{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "07ab75af",
            "metadata": {},
            "source": [
                "# Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d40f833d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %pip install -U torch-2.3.0-cp311-cp311-win_amd64.whl\n",
                "# %pip install -U sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc4c3dfb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from python import FileManager\n",
                "from python import WordCleaner\n",
                "from python import Indexer\n",
                "from python import Matcher\n",
                "from python import Evaluater\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from nltk.tokenize import word_tokenize\n",
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f86b2947",
            "metadata": {},
            "source": [
                "# Dataset Manipulation "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "069c2740",
            "metadata": {},
            "source": [
                "## Load Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91d69ec5",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer()\n",
                "svd = TruncatedSVD(n_components=100, algorithm=\"arpack\")\n",
                "dataset = FileManager.csv_to_dict(\"wikir/RL3.csv\")\n",
                "datasets = [dataset]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "15979398",
            "metadata": {},
            "source": [
                "### The Ultimate Loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b3281041",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer()\n",
                "svd = TruncatedSVD(n_components=50, algorithm=\"arpack\")\n",
                "\n",
                "\n",
                "dataset = {}\n",
                "for i in range(0,4):\n",
                "    dataset = dataset | FileManager.csv_to_dict(f\"lotte/lemlot{i}.csv\")\n",
                "datasets = [dataset]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae375500",
            "metadata": {},
            "source": [
                "## Remove stop words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65723f2b",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "filtered_dataset = {}\n",
                "for key in dataset:\n",
                "    if int(key) % 100 == 0:\n",
                "        print(key)\n",
                "    filtered_dataset[key] = WordCleaner.remove_stop_words(dataset[key])\n",
                "datasets.append(filtered_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45492cbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "processed_dataset = {}\n",
                "for key in dataset:\n",
                "    processed_dataset[key] = WordCleaner.process_capital_punctuation(dataset[key])\n",
                "datasets.append(processed_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6621f9e3",
            "metadata": {},
            "source": [
                "## Stem"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4377f840",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "stemmed_dataset = {}\n",
                "for row in dataset:\n",
                "    stemmed_dataset[row] = WordCleaner.stem(dataset[row], \"Snowball\")\n",
                "datasets.append(stemmed_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8bd7a58d",
            "metadata": {},
            "source": [
                "## Lemmatize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43fe0f11",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "lemmad_dataset = {}\n",
                "for row in dataset:\n",
                "    lemmad_dataset[row] = WordCleaner.lemmatize(dataset[row])\n",
                "datasets.append(lemmad_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9c0bcf1",
            "metadata": {},
            "source": [
                "## Synonym Map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7af5cf99",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "mapped_dataset = WordCleaner.synonym_map_corpus(dataset)\n",
                "datasets.append(mapped_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75f368df",
            "metadata": {},
            "source": [
                "## Calculating tf-idf for the document"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48b60ed8",
            "metadata": {},
            "source": [
                "### using Scikit Learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "98eff815",
            "metadata": {},
            "outputs": [],
            "source": [
                "tfidf_matrix = Indexer.calculate_tf_idf(datasets[-1], vectorizer)\n",
                "dataset_keys = list(datasets[-1].keys())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d39d29e5",
            "metadata": {},
            "source": [
                "#### LSA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29e68704",
            "metadata": {},
            "outputs": [],
            "source": [
                "svd = TruncatedSVD(n_components=50, algorithm=\"arpack\")\n",
                "lsa_matrix = Indexer.calculate_lsa(tfidf_matrix,svd)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9cdd8aa",
            "metadata": {},
            "source": [
                "# Query Manipulation "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb756f25",
            "metadata": {},
            "source": [
                "## Manual Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94fad178",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"hello sister where is your hijab\"\n",
                "query = word_tokenize(query)\n",
                "query = WordCleaner.remove_stop_words(query)\n",
                "query = WordCleaner.stem(query, 'Snowball')\n",
                "# query = WordCleaner.lemmatize(query)\n",
                "query = [WordCleaner.get_unified_synonym(word) for word in query]\n",
                "print(query)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7079d6ac",
            "metadata": {},
            "source": [
                "### Calculate TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "859dc7da",
            "metadata": {},
            "outputs": [],
            "source": [
                "matrix = Indexer.calculate_doc_tf_idf([\" \".join(query)],vectorizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6fcb0868",
            "metadata": {},
            "outputs": [],
            "source": [
                "matrix = Indexer.calculate_doc_lsa([\" \".join(query)],svd)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d08b71d",
            "metadata": {},
            "source": [
                "### Calculate Cosine Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd8427af",
            "metadata": {},
            "outputs": [],
            "source": [
                "similar_rows = Matcher.get_query_answers(lsa_matrix,matrix,dataset_keys,0.5)\n",
                "\n",
                "for row in similar_rows.items():\n",
                "    print(row)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "17659ba6",
            "metadata": {},
            "source": [
                "## Evaluation Queries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db528201",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/queries.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c718b9f1",
            "metadata": {},
            "source": [
                "### Lotte queries loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d15389a",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.tsv_to_dict(\"lotte/questions.forum.tsv\") #| FileManager.tsv_to_dict(\"lotte/questions.search.tsv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29aad331",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.jsonl_to_tsv(\"lotte/qas.forum.jsonl\", \"qrels0\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63e5cbd5",
            "metadata": {},
            "source": [
                "### Text Processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9dcc6d1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO get unified synonym needs testing\n",
                "# for key in queries.keys():\n",
                "#     queries[key] = WordCleaner.remove_stop_words(queries[key])\n",
                "# for key in queries.keys():\n",
                "#     queries[key] = [WordCleaner.get_unified_synonym(word) for word in queries[key]]\n",
                "\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.remove_stop_words(queries[key])\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.process_capital_punctuation(queries[key])\n",
                "# for key in queries.keys():\n",
                "#     queries[key] = WordCleaner.stem(queries[key], \"Snowball\")\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.lemmatize(queries[key])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0320dc0f",
            "metadata": {},
            "source": [
                "### Calculate TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd3903f0",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries_matrices = {}\n",
                "for key in queries.keys():\n",
                "    queries_matrices[key] = Indexer.calculate_doc_tf_idf([\" \".join(queries[key])],vectorizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d03a5b5e",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries_matrices2 = {}\n",
                "for key in queries_matrices.keys():\n",
                "    queries_matrices2[key] = Indexer.calculate_doc_lsa(queries_matrices[key],svd)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7d46388",
            "metadata": {},
            "source": [
                "### Calculate Cosine Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "562157b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "queriesAnswers = {}\n",
                "for key in queries.keys():\n",
                "    queriesAnswers[key] = Matcher.get_query_answers_optimized(tfidf_matrix,queries_matrices[key],dataset_keys,0.9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ead94063",
            "metadata": {},
            "outputs": [],
            "source": [
                "queriesAnswers = {}\n",
                "for key in queries.keys():\n",
                "    queriesAnswers[key] = Matcher.get_query_answers_optimized(lsa_matrix,queries_matrices2[key],dataset_keys,0.95)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1edcaba6",
            "metadata": {},
            "source": [
                "# Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a6412b55",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"lotte/qrels0\",\"lotte_rl_SVA3.run\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "11d3dcab",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"lotte/qrels0\",\"lotte_rl_SVA1.run\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a0b27ea4",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"lotte/qrels0\",\"lotte_rl_SVA2.run\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3311fe2",
            "metadata": {},
            "source": [
                "# Write To Files"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "731ad4ac",
            "metadata": {},
            "source": [
                "## Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13c2eea2",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_dataset_to_file(\"lemlot2.csv\",datasets[-1])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d8a34c7e",
            "metadata": {},
            "source": [
                "## Run File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d525af42",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_runfile_to_file(\"lotte_rl_SVA3.run\",queries,queriesAnswers)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7bdbe7ab",
            "metadata": {},
            "source": [
                "## Model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3094d4e4",
            "metadata": {},
            "source": [
                "### Write"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "379b09d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_model_to_drive(\"wikir\",vectorizer,svd, dataset_keys, tfidf_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34d03817",
            "metadata": {},
            "source": [
                "### Read"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8da4498",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer, svd, dataset_keys, tfidf_matrix = FileManager.load_model_from_drive(\"lotte\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
