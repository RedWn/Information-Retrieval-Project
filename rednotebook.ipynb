{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "07ab75af",
            "metadata": {},
            "source": [
                "# Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc4c3dfb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from python import FileManager\n",
                "from python import WordCleaner\n",
                "from python import Indexer\n",
                "from python import Matcher\n",
                "from python import Evaluater\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from nltk.tokenize import word_tokenize\n",
                "from tqdm import tqdm\n",
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f86b2947",
            "metadata": {},
            "source": [
                "# Dataset Manipulation "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "069c2740",
            "metadata": {},
            "source": [
                "## Load Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91d69ec5",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer()\n",
                "dataset = FileManager.csv_to_dict(\"wikirRML.csv\")\n",
                "datasets = [dataset]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "15979398",
            "metadata": {},
            "source": [
                "### The Ultimate Loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b3281041",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer()\n",
                "\n",
                "dataset = {}\n",
                "for i in tqdm(range(0,4)):\n",
                "    dataset = dataset | FileManager.csv_to_dict(f\"wikir/csv/RL{i}.csv\")\n",
                "datasets = [dataset]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae375500",
            "metadata": {},
            "source": [
                "## Remove stop words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65723f2b",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "filtered_dataset = {}\n",
                "for key in dataset:\n",
                "    filtered_dataset[key] = WordCleaner.remove_stop_words(dataset[key])\n",
                "datasets.append(filtered_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45492cbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "processed_dataset = {}\n",
                "for key in dataset:\n",
                "    processed_dataset[key] = WordCleaner.process_capital_punctuation(dataset[key])\n",
                "datasets.append(processed_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6621f9e3",
            "metadata": {},
            "source": [
                "## Stem"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4377f840",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "stemmed_dataset = {}\n",
                "for row in dataset:\n",
                "    stemmed_dataset[row] = WordCleaner.stem(dataset[row], \"Snowball\")\n",
                "datasets.append(stemmed_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8bd7a58d",
            "metadata": {},
            "source": [
                "## Lemmatize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43fe0f11",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "lemmad_dataset = {}\n",
                "for row in dataset:\n",
                "    lemmad_dataset[row] = WordCleaner.lemmatize(dataset[row])\n",
                "datasets.append(lemmad_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9c0bcf1",
            "metadata": {},
            "source": [
                "## Synonym Map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd37243b",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "no_singles_dataset = {}\n",
                "for key in tqdm(dataset):\n",
                "    no_singles_dataset[key] = WordCleaner.remove_single_letters(dataset[key])\n",
                "datasets.append(no_singles_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a65dbf2",
            "metadata": {},
            "outputs": [],
            "source": [
                "from multiprocessing import Pool\n",
                "from tqdm import tqdm\n",
                "\n",
                "dataset = datasets[-1]\n",
                "mapped_2 = {}\n",
                "\n",
                "# Create a pool of workers\n",
                "with Pool() as p:\n",
                "    # Wrap your iterator (dataset) with tqdm for a progress bar\n",
                "    for row in tqdm(dataset):\n",
                "        # Apply the function to each word in the row in parallel\n",
                "        mapped_2[row] = p.map(WordCleaner.get_unified_synonym_2, dataset[row])\n",
                "datasets.append(mapped_2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75f368df",
            "metadata": {},
            "source": [
                "## Calculating tf-idf for the document"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48b60ed8",
            "metadata": {},
            "source": [
                "### using Scikit Learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "98eff815",
            "metadata": {},
            "outputs": [],
            "source": [
                "tfidf_matrix = Indexer.calculate_tf_idf(datasets[-1], vectorizer)\n",
                "dataset_keys = list(datasets[-1].keys())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d39d29e5",
            "metadata": {},
            "source": [
                "#### LSA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29e68704",
            "metadata": {},
            "outputs": [],
            "source": [
                "svd = TruncatedSVD(n_components=6, algorithm=\"arpack\")\n",
                "lsa_matrix = Indexer.calculate_lsa(tfidf_matrix,svd)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aded0dab",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "terms = vectorizer.get_feature_names_out()\n",
                "\n",
                "topics_dict = {}\n",
                "for i, comp in enumerate(tqdm(svd.components_)):\n",
                "    terms_comp = zip(terms, comp)\n",
                "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
                "    topics_dict[i] = sorted_terms\n",
                "\n",
                "for key, value_list in topics_dict.items():\n",
                "    if isinstance(value_list, list) and value_list:\n",
                "        # Extract the first value from each tuple\n",
                "        first_values = [tup[0] for tup in value_list]\n",
                "        topics_dict[key] = \" \".join(first_values)\n",
                "        \n",
                "# df = pd.DataFrame(topics_dict)\n",
                "# df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d85a84f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import umap\n",
                "# import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "\n",
                "# embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(lsa_matrix)\n",
                "\n",
                "\n",
                "from sklearn.decomposition import PCA\n",
                " \n",
                "pca = PCA(2)\n",
                "pca.fit(lsa_matrix)\n",
                " \n",
                "pca_matrix = pca.transform(lsa_matrix)\n",
                "pca_data = pd.DataFrame(pca.transform(lsa_matrix))\n",
                "# print(pca_data.head())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bcf6686",
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Scaling the data to normalize\n",
                "model = KMeans(n_clusters=6).fit(pca_matrix)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(28,20))\n",
                "scatter = ax.scatter(pca_matrix[:, 0], pca_matrix[:, 1], \n",
                "c = model.labels_.astype(float),\n",
                "s = 20, # size\n",
                "edgecolor='none'\n",
                ")\n",
                "\n",
                "# produce a legend with the unique colors from the scatter\n",
                "legend1 = ax.legend(*scatter.legend_elements(),\n",
                "                    loc=\"lower left\", title=\"Topics\")\n",
                "ax.add_artist(legend1)\n",
                "\n",
                "for t, l in zip(legend1.texts, list(topics_dict.values()),):\n",
                "    t.set_text(l)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67652de2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "np.save(\"lsa_matrixmapnew\",lsa_matrix)\n",
                "# embedding = np.load(\"embedding.np.npy\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9cdd8aa",
            "metadata": {},
            "source": [
                "# Query Manipulation "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb756f25",
            "metadata": {},
            "source": [
                "## Manual Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94fad178",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"she co founded the phillips collection with her husband duncan phillips she was born marjorie acker in bourbon indiana she was the sister to six other siblings her parents were charles ernest acker and alice beal she was raised in ossining new york phillips started drawing as a child her uncles were reynolds beal and gifford beal both men noticed phillips artistic ability and suggested she pursue art as a career path she began attending the art students league in 1915 and graduated in 1918 she studied under boardman robinson marjorie phillips has the unmistakable style of the born painter duncan phillips phillips is quoted as stating that she didn t want to paint depressing pictures she painted primarily landscapes and still life works despite living a socialite lifestyle alongside her husband phillips made the effort to paint every morning in her washington d c studio she attended an art exhibition for duncan phillips at the century association in january 1921 she met duncan and the two married in october of that year duncan was an art collector and the couple expanded their collecting phillips moved to washington d c and into duncan s dupont circle mansion duncan s mother\"\n",
                "query = word_tokenize(query)\n",
                "query = WordCleaner.remove_stop_words(query)\n",
                "# query = WordCleaner.stem(query, 'Snowball')\n",
                "query = WordCleaner.lemmatize(query)\n",
                "# query = [WordCleaner.get_unified_synonym(word) for word in query]\n",
                "print(query)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7079d6ac",
            "metadata": {},
            "source": [
                "### Calculate TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "859dc7da",
            "metadata": {},
            "outputs": [],
            "source": [
                "matrix = Indexer.calculate_doc_tf_idf([\" \".join(query)],vectorizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6fcb0868",
            "metadata": {},
            "outputs": [],
            "source": [
                "matrix = Indexer.calculate_doc_lsa(matrix,svd)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d08b71d",
            "metadata": {},
            "source": [
                "### Calculate Cosine Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd8427af",
            "metadata": {},
            "outputs": [],
            "source": [
                "similar_rows = Matcher.get_query_answers(lsa_matrix,matrix,dataset_keys,0.9)\n",
                "\n",
                "for row in similar_rows.items():\n",
                "    print(row)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "17659ba6",
            "metadata": {},
            "source": [
                "## Evaluation Queries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db528201",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/testing/queries.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c718b9f1",
            "metadata": {},
            "source": [
                "### Lotte queries loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d15389a",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/queries.csv\",delimiter=\"\\t\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63e5cbd5",
            "metadata": {},
            "source": [
                "### Text Processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9dcc6d1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from multiprocessing import Pool\n",
                "\n",
                "for key in queries.keys():\n",
                "    # queries[key] = [WordCleaner.get_unified_synonym(word) for word in queries[key]]\n",
                "    queries[key] = WordCleaner.remove_stop_words(queries[key])\n",
                "#     queries[key] = WordCleaner.process_capital_punctuation(queries[key])\n",
                "# with Pool() as p:\n",
                "#     for row in tqdm(queries):\n",
                "#         queries[row] = p.map(WordCleaner.get_unified_synonym_2, queries[row])\n",
                "#     queries[key] = WordCleaner.stem(queries[key], \"Snowball\")\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.lemmatize(queries[key])\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.remove_single_letters(queries[key])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0320dc0f",
            "metadata": {},
            "source": [
                "### Calculate TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "260722c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries_matrices = {}\n",
                "for key in tqdm(queries.keys()):\n",
                "    queries_matrices[key] = Indexer.calculate_doc_tf_idf([\" \".join(queries[key])],vectorizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7d46388",
            "metadata": {},
            "source": [
                "### Calculate Cosine Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "562157b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "queriesAnswers = {}\n",
                "for key in tqdm(queries_matrices.keys()):\n",
                "    queriesAnswers[key] = Matcher.get_query_answers(tfidf_matrix,queries_matrices[key],dataset_keys,0.35)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36504471",
            "metadata": {},
            "outputs": [],
            "source": [
                "queriesAnswers = {}\n",
                "for key in tqdm(queries_matrices.keys()):\n",
                "    queriesAnswers[key] = Matcher.get_query_answers(tfidf_matrix,queries_matrices[key],dataset_keys,0.35)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1edcaba6",
            "metadata": {},
            "source": [
                "# Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "11d3dcab",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirNRML35.run\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71bc01c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirRMLN35.run\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b968cff3",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirRML35.run\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3311fe2",
            "metadata": {},
            "source": [
                "# Write To Files"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "731ad4ac",
            "metadata": {},
            "source": [
                "## Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13c2eea2",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_dataset_to_file(\"wikirRML.csv\",datasets[-1])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d8a34c7e",
            "metadata": {},
            "source": [
                "## Run File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d525af42",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_runfile_to_file(\"TwikirRMLN35.run\",queries,queriesAnswers,max_relevance=2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7bdbe7ab",
            "metadata": {},
            "source": [
                "## Model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3094d4e4",
            "metadata": {},
            "source": [
                "### Write"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "379b09d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_model_to_drive(\"wikir_RMLN\",vectorizer, dataset_keys, tfidf_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34d03817",
            "metadata": {},
            "source": [
                "### Read"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8da4498",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer, dataset_keys, tfidf_matrix = FileManager.load_model_from_drive(\"wikir_RMLN\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
