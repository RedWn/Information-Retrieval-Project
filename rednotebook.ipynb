{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "07ab75af",
            "metadata": {},
            "source": [
                "# Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc4c3dfb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from python import FileManager\n",
                "from python import WordCleaner\n",
                "from python import Indexer\n",
                "from python import Matcher\n",
                "from python import Evaluater\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from nltk.tokenize import word_tokenize\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "import numpy as np\n",
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f86b2947",
            "metadata": {},
            "source": [
                "# Dataset Manipulation "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "069c2740",
            "metadata": {},
            "source": [
                "## Load Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91d69ec5",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer()\n",
                "dataset = FileManager.csv_to_dict(\"./wikir/csv/wikir.csv\")\n",
                "datasets = [dataset]\n",
                "dataset_keys = list(datasets[-1].keys())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "15979398",
            "metadata": {},
            "source": [
                "### The Ultimate Loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b3281041",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer()\n",
                "\n",
                "dataset = {}\n",
                "for i in tqdm(range(0,4)):\n",
                "    dataset = dataset | FileManager.csv_to_dict(f\"wikir/csv/RL{i}.csv\")\n",
                "datasets = [dataset]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae375500",
            "metadata": {},
            "source": [
                "## Remove stop words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65723f2b",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "filtered_dataset = {}\n",
                "for key in dataset:\n",
                "    filtered_dataset[key] = WordCleaner.remove_stop_words(dataset[key])\n",
                "datasets.append(filtered_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45492cbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "processed_dataset = {}\n",
                "for key in dataset:\n",
                "    processed_dataset[key] = WordCleaner.process_capital_punctuation(dataset[key])\n",
                "datasets.append(processed_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6621f9e3",
            "metadata": {},
            "source": [
                "## Stem"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4377f840",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "stemmed_dataset = {}\n",
                "for row in dataset:\n",
                "    stemmed_dataset[row] = WordCleaner.stem(dataset[row], \"Snowball\")\n",
                "datasets.append(stemmed_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8bd7a58d",
            "metadata": {},
            "source": [
                "## Lemmatize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43fe0f11",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "lemmad_dataset = {}\n",
                "for row in dataset:\n",
                "    lemmad_dataset[row] = WordCleaner.lemmatize(dataset[row])\n",
                "datasets.append(lemmad_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9c0bcf1",
            "metadata": {},
            "source": [
                "## Synonym Map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd37243b",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "no_singles_dataset = {}\n",
                "for key in tqdm(dataset):\n",
                "    no_singles_dataset[key] = WordCleaner.remove_single_letters(dataset[key])\n",
                "datasets.append(no_singles_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a65dbf2",
            "metadata": {},
            "outputs": [],
            "source": [
                "from multiprocessing import Pool\n",
                "from tqdm import tqdm\n",
                "\n",
                "dataset = datasets[-1]\n",
                "mapped_2 = {}\n",
                "\n",
                "# Create a pool of workers\n",
                "with Pool() as p:\n",
                "    # Wrap your iterator (dataset) with tqdm for a progress bar\n",
                "    for row in tqdm(dataset):\n",
                "        # Apply the function to each word in the row in parallel\n",
                "        mapped_2[row] = p.map(WordCleaner.get_unified_synonym_2, dataset[row])\n",
                "datasets.append(mapped_2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75f368df",
            "metadata": {},
            "source": [
                "## Calculating tf-idf for the document"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48b60ed8",
            "metadata": {},
            "source": [
                "### using Scikit Learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "98eff815",
            "metadata": {},
            "outputs": [],
            "source": [
                "tfidf_matrix = Indexer.calculate_tf_idf(datasets[-1], vectorizer)\n",
                "dataset_keys = list(datasets[-1].keys())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d39d29e5",
            "metadata": {},
            "source": [
                "#### LSA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29e68704",
            "metadata": {},
            "outputs": [],
            "source": [
                "svd = TruncatedSVD(n_components=6, algorithm=\"arpack\")\n",
                "lsa_matrix = Indexer.calculate_lsa(tfidf_matrix,svd)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aded0dab",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "terms = vectorizer.get_feature_names_out()\n",
                "\n",
                "topics_dict = {}\n",
                "for i, comp in enumerate(tqdm(svd.components_)):\n",
                "    terms_comp = zip(terms, comp)\n",
                "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
                "    topics_dict[i] = sorted_terms\n",
                "\n",
                "for key, value_list in topics_dict.items():\n",
                "    if isinstance(value_list, list) and value_list:\n",
                "        # Extract the first value from each tuple\n",
                "        first_values = [tup[0] for tup in value_list]\n",
                "        topics_dict[key] = \" \".join(first_values)\n",
                "        \n",
                "# df = pd.DataFrame(topics_dict)\n",
                "# df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d85a84f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import umap\n",
                "# import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "\n",
                "# embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(lsa_matrix)\n",
                "\n",
                "\n",
                "from sklearn.decomposition import PCA\n",
                " \n",
                "pca = PCA(2)\n",
                "pca.fit(lsa_matrix)\n",
                " \n",
                "pca_matrix = pca.transform(lsa_matrix)\n",
                "pca_data = pd.DataFrame(pca.transform(lsa_matrix))\n",
                "# print(pca_data.head())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bcf6686",
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Scaling the data to normalize\n",
                "model = KMeans(n_clusters=6).fit(pca_matrix)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(28,20))\n",
                "scatter = ax.scatter(pca_matrix[:, 0], pca_matrix[:, 1], \n",
                "c = model.labels_.astype(float),\n",
                "s = 20, # size\n",
                "edgecolor='none'\n",
                ")\n",
                "\n",
                "# produce a legend with the unique colors from the scatter\n",
                "legend1 = ax.legend(*scatter.legend_elements(),\n",
                "                    loc=\"lower left\", title=\"Topics\")\n",
                "ax.add_artist(legend1)\n",
                "\n",
                "for t, l in zip(legend1.texts, list(topics_dict.values()),):\n",
                "    t.set_text(l)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67652de2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "np.save(\"lsa_matrixmapnew\",lsa_matrix)\n",
                "# embedding = np.load(\"embedding.np.npy\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9cdd8aa",
            "metadata": {},
            "source": [
                "# Query Manipulation "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb756f25",
            "metadata": {},
            "source": [
                "## Manual Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94fad178",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"she co founded the phillips collection with her husband duncan phillips she was born marjorie acker in bourbon indiana she was the sister to six other siblings her parents were charles ernest acker and alice beal she was raised in ossining new york phillips started drawing as a child her uncles were reynolds beal and gifford beal both men noticed phillips artistic ability and suggested she pursue art as a career path she began attending the art students league in 1915 and graduated in 1918 she studied under boardman robinson marjorie phillips has the unmistakable style of the born painter duncan phillips phillips is quoted as stating that she didn t want to paint depressing pictures she painted primarily landscapes and still life works despite living a socialite lifestyle alongside her husband phillips made the effort to paint every morning in her washington d c studio she attended an art exhibition for duncan phillips at the century association in january 1921 she met duncan and the two married in october of that year duncan was an art collector and the couple expanded their collecting phillips moved to washington d c and into duncan s dupont circle mansion duncan s mother\"\n",
                "query = word_tokenize(query)\n",
                "query = WordCleaner.remove_stop_words(query)\n",
                "# query = WordCleaner.stem(query, 'Snowball')\n",
                "query = WordCleaner.lemmatize(query)\n",
                "# query = [WordCleaner.get_unified_synonym(word) for word in query]\n",
                "print(query)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7079d6ac",
            "metadata": {},
            "source": [
                "### Calculate TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "859dc7da",
            "metadata": {},
            "outputs": [],
            "source": [
                "matrix = Indexer.calculate_doc_tf_idf([\" \".join(query)],vectorizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6fcb0868",
            "metadata": {},
            "outputs": [],
            "source": [
                "matrix = Indexer.calculate_doc_lsa(matrix,svd)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d08b71d",
            "metadata": {},
            "source": [
                "### Calculate Cosine Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd8427af",
            "metadata": {},
            "outputs": [],
            "source": [
                "similar_rows = Matcher.get_query_answers(lsa_matrix,matrix,dataset_keys,0.9)\n",
                "\n",
                "for row in similar_rows.items():\n",
                "    print(row)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "17659ba6",
            "metadata": {},
            "source": [
                "## Evaluation Queries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db528201",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/testing/queries.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c718b9f1",
            "metadata": {},
            "source": [
                "### Lotte queries loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d15389a",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/queries.csv\",delimiter=\"\\t\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63e5cbd5",
            "metadata": {},
            "source": [
                "### Text Processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9dcc6d1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from multiprocessing import Pool\n",
                "\n",
                "for key in queries.keys():\n",
                "    # queries[key] = [WordCleaner.get_unified_synonym(word) for word in queries[key]]\n",
                "    queries[key] = WordCleaner.remove_stop_words(queries[key])\n",
                "#     queries[key] = WordCleaner.process_capital_punctuation(queries[key])\n",
                "# with Pool() as p:\n",
                "#     for row in tqdm(queries):\n",
                "#         queries[row] = p.map(WordCleaner.get_unified_synonym_2, queries[row])\n",
                "#     queries[key] = WordCleaner.stem(queries[key], \"Snowball\")\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.lemmatize(queries[key])\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.remove_single_letters(queries[key])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0320dc0f",
            "metadata": {},
            "source": [
                "### Calculate TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "260722c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries_matrices = {}\n",
                "for key in tqdm(queries.keys()):\n",
                "    queries_matrices[key] = Indexer.calculate_doc_tf_idf([\" \".join(queries[key])],vectorizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7d46388",
            "metadata": {},
            "source": [
                "### Calculate Cosine Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "562157b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "queriesAnswers = {}\n",
                "for key in tqdm(queries_matrices.keys()):\n",
                "    queriesAnswers[key] = Matcher.get_query_answers(tfidf_matrix,queries_matrices[key],dataset_keys,0.35)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36504471",
            "metadata": {},
            "outputs": [],
            "source": [
                "queriesAnswers = {}\n",
                "for key in tqdm(queries_matrices.keys()):\n",
                "    queriesAnswers[key] = Matcher.get_query_answers(tfidf_matrix,queries_matrices[key],dataset_keys,0.35)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1edcaba6",
            "metadata": {},
            "source": [
                "# Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "11d3dcab",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirNRML35.run\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71bc01c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirRMLN35.run\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b968cff3",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirRML35.run\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3311fe2",
            "metadata": {},
            "source": [
                "# Write To Files"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "731ad4ac",
            "metadata": {},
            "source": [
                "## Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13c2eea2",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_dataset_to_file(\"wikirRML.csv\",datasets[-1])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d8a34c7e",
            "metadata": {},
            "source": [
                "## Run File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d525af42",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_runfile_to_file(\"TwikirRMLN35.run\",queries,queriesAnswers,max_relevance=2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7bdbe7ab",
            "metadata": {},
            "source": [
                "## Model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3094d4e4",
            "metadata": {},
            "source": [
                "### Write"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "379b09d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_model_to_drive(\"wikir_RMLN\",vectorizer, dataset_keys, tfidf_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34d03817",
            "metadata": {},
            "source": [
                "### Read"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8da4498",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer, dataset_keys, tfidf_matrix = FileManager.load_model_from_drive(\"wikir_RMLN\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b6b2e89f",
            "metadata": {},
            "source": [
                "# Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f59aeb70",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import BertTokenizer, BertModel\n",
                "from tqdm import tqdm\n",
                "\n",
                "model = BertModel.from_pretrained('./model',ignore_mismatched_sizes=True)\n",
                "tokenizer = BertTokenizer.from_pretrained('./model',ignore_mismatched_sizes=True)\n",
                "dataset = datasets[-1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a95e9940",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/testing/queries.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "974869b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenized_documents = {doc_id: tokenizer(' '.join(words), return_tensors='pt') for doc_id, words in tqdm(dataset.items())}\n",
                "\n",
                "tokenized_queries = {query_id: tokenizer(' '.join(words), return_tensors='pt') for query_id, words in queries.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e52bf639",
            "metadata": {},
            "outputs": [],
            "source": [
                "query_vectors = {query_id: model(**words)[0][0][0] for query_id, words in tokenized_queries.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3eca9965",
            "metadata": {},
            "outputs": [],
            "source": [
                "document_vectors = {doc_id: model(**words)[0][0][0] for doc_id, words in tqdm(tokenized_documents.items())}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4d65a2f4",
            "metadata": {},
            "outputs": [],
            "source": [
                "document_vector = {'1781133': model(**tokenized_documents[\"1781133\"])[0][0][0]}\n",
                "document_vector"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dfffbad2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "device = torch.device('cuda')\n",
                "model = model.to(device)\n",
                "\n",
                "document_vectors = {}\n",
                "batch_size = 512  # Adjust this based on your VRAM availability\n",
                "\n",
                "doc_ids = list(tokenized_documents.keys())\n",
                "doc_batches = [doc_ids[i:i + batch_size] for i in range(0, len(doc_ids), batch_size)]\n",
                "\n",
                "for i,batch in enumerate(doc_batches):\n",
                "    print(f\"Processing batch {i} out of {len(doc_batches)}\")\n",
                "    batch_dict = {doc_id: tokenized_documents[doc_id] for doc_id in batch}\n",
                "    with torch.no_grad():\n",
                "        for doc_id, words in batch_dict.items():\n",
                "            document_vectors[doc_id] = model(**words.to(device))[0][0][0]\n",
                "            torch.cuda.empty_cache()  # Free up unused memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7edc2f0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert BERT embeddings to 2D numpy arrays\n",
                "document_vectors_np = {doc_id: doc_vector.detach().cpu().numpy().reshape(1, -1) for doc_id, doc_vector in document_vectors.items()}\n",
                "query_vectors_np = {query_id: query_vector.detach().cpu().numpy().reshape(1, -1) for query_id, query_vector in query_vectors.items()}\n",
                "\n",
                "# Create corpus_matrix and query_matrix\n",
                "corpus_matrix = np.vstack(list(document_vectors_np.values()))\n",
                "query_matrix = np.vstack(list(query_vectors_np.values()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f9cd1f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries_answers = {}\n",
                "for key in queries.keys():\n",
                "    queries_answers[key] = Matcher.get_query_answers(corpus_matrix, query_vectors_np[key], dataset_keys, 0.85)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "89c95f3f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write to run file\n",
                "FileManager.write_runfile_to_file('bert85.run', queries, queries_answers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "216e37fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"bert85.run\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7717c457",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manual Query\n",
                "query_words = [\"yanni\"]\n",
                "\n",
                "# Tokenize and convert your query to IDs\n",
                "tokenized_query = tokenizer.tokenize(' '.join(query_words))\n",
                "indexed_query = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
                "\n",
                "# Move indexed_query to the correct device and calculate the query vector\n",
                "indexed_query = torch.tensor([indexed_query]).to(device)\n",
                "query_vector = model(indexed_query)[0][0][0]\n",
                "\n",
                "# Move query_vector to the CPU, detach it from the computation graph, and convert it to a numpy array\n",
                "query_matrix = query_vector.detach().cpu().numpy().reshape(1, -1)\n",
                "\n",
                "# Use your function to get the most similar documents\n",
                "similar_docs = Matcher.get_query_answers_optimized(corpus_matrix, query_matrix, dataset_keys, 0.55)\n",
                "\n",
                "# Print the IDs of the top 5 most similar documents\n",
                "for i, (doc_id, score) in enumerate(list(similar_docs.items())[:10]):\n",
                "    print(f\"Rank {i+1}, Document ID: {doc_id}, Similarity Score: {score}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
