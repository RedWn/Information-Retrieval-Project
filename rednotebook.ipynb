{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "07ab75af",
            "metadata": {},
            "source": [
                "# Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc4c3dfb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from python import FileManager\n",
                "from python import WordCleaner\n",
                "from python import Indexer\n",
                "from python import Matcher\n",
                "from python import Evaluater\n",
                "from python import Clusterer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from nltk.tokenize import word_tokenize\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "import numpy as np\n",
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f86b2947",
            "metadata": {},
            "source": [
                "# Dataset Manipulation "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "069c2740",
            "metadata": {},
            "source": [
                "## Load Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91d69ec5",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer()\n",
                "dataset = FileManager.csv_to_dict(\"lotte/collection.tsv\",delimiter=\"\\t\")\n",
                "datasets = [dataset]\n",
                "dataset_keys = list(datasets[-1].keys())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "15979398",
            "metadata": {},
            "source": [
                "### The Ultimate Loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b3281041",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer()\n",
                "\n",
                "dataset = {}\n",
                "for i in range(0,4):\n",
                "    dataset = dataset | FileManager.csv_to_dict(f\"lotte/lemlot{i}.csv\")\n",
                "datasets = [dataset]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae375500",
            "metadata": {},
            "source": [
                "## Remove stop words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65723f2b",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "filtered_dataset = {}\n",
                "for key in dataset:\n",
                "    filtered_dataset[key] = WordCleaner.remove_stop_words(dataset[key])\n",
                "datasets.append(filtered_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45492cbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[0]\n",
                "processed_dataset = {}\n",
                "for key in tqdm(dataset):\n",
                "    processed_dataset[key] = \" \".join(WordCleaner.process_capital_punctuation(dataset[key]))\n",
                "datasets.append(processed_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6621f9e3",
            "metadata": {},
            "source": [
                "## Stem"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4377f840",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "stemmed_dataset = {}\n",
                "for row in dataset:\n",
                "    stemmed_dataset[row] = WordCleaner.stem(dataset[row], \"Snowball\")\n",
                "datasets.append(stemmed_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8bd7a58d",
            "metadata": {},
            "source": [
                "## Lemmatize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43fe0f11",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "lemmad_dataset = {}\n",
                "for row in dataset:\n",
                "    lemmad_dataset[row] = WordCleaner.lemmatize(dataset[row])\n",
                "datasets.append(lemmad_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9c0bcf1",
            "metadata": {},
            "source": [
                "## Synonym Map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd37243b",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "no_singles_dataset = {}\n",
                "for key in tqdm(dataset):\n",
                "    no_singles_dataset[key] = WordCleaner.remove_single_letters(dataset[key])\n",
                "datasets.append(no_singles_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a65dbf2",
            "metadata": {},
            "outputs": [],
            "source": [
                "from multiprocessing import Pool\n",
                "from tqdm import tqdm\n",
                "\n",
                "dataset = datasets[-1]\n",
                "mapped_2 = {}\n",
                "\n",
                "# Create a pool of workers\n",
                "with Pool() as p:\n",
                "    # Wrap your iterator (dataset) with tqdm for a progress bar\n",
                "    for row in tqdm(dataset):\n",
                "        # Apply the function to each word in the row in parallel\n",
                "        mapped_2[row] = p.map(WordCleaner.get_unified_synonym_2, dataset[row])\n",
                "datasets.append(mapped_2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75f368df",
            "metadata": {},
            "source": [
                "## Calculating tf-idf for the document"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48b60ed8",
            "metadata": {},
            "source": [
                "### using Scikit Learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "98eff815",
            "metadata": {},
            "outputs": [],
            "source": [
                "tfidf_matrix = Indexer.calculate_tf_idf(datasets[-1], vectorizer)\n",
                "dataset_keys = list(datasets[-1].keys())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5abfc76b",
            "metadata": {},
            "source": [
                "## Clustering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47e4bde7",
            "metadata": {},
            "outputs": [],
            "source": [
                "c = Clusterer.Clusterer(tfidf_matrix,6)\n",
                "c.plot(size=(21, 15),topics=c.getTopics(datasets[-1],keys=dataset_keys))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b601a4cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "import enthought.mayavi.mlab as mylab\n",
                "x, y, z, value = np.random.random((4, 40))\n",
                "mylab.points3d(x, y, z, value)\n",
                "mylab.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb756f25",
            "metadata": {},
            "source": [
                "## Manual Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94fad178",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"she co founded the phillips collection with her husband duncan phillips she was born marjorie acker in bourbon indiana she was the sister to six other siblings her parents were charles ernest acker and alice beal she was raised in ossining new york phillips started drawing as a child her uncles were reynolds beal and gifford beal both men noticed phillips artistic ability and suggested she pursue art as a career path she began attending the art students league in 1915 and graduated in 1918 she studied under boardman robinson marjorie phillips has the unmistakable style of the born painter duncan phillips phillips is quoted as stating that she didn t want to paint depressing pictures she painted primarily landscapes and still life works despite living a socialite lifestyle alongside her husband phillips made the effort to paint every morning in her washington d c studio she attended an art exhibition for duncan phillips at the century association in january 1921 she met duncan and the two married in october of that year duncan was an art collector and the couple expanded their collecting phillips moved to washington d c and into duncan s dupont circle mansion duncan s mother\"\n",
                "query = word_tokenize(query)\n",
                "query = WordCleaner.remove_stop_words(query)\n",
                "# query = WordCleaner.stem(query, 'Snowball')\n",
                "query = WordCleaner.lemmatize(query)\n",
                "# query = [WordCleaner.get_unified_synonym(word) for word in query]\n",
                "print(query)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7079d6ac",
            "metadata": {},
            "source": [
                "### Calculate TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "859dc7da",
            "metadata": {},
            "outputs": [],
            "source": [
                "matrix = Indexer.calculate_doc_tf_idf([\" \".join(query)],vectorizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d08b71d",
            "metadata": {},
            "source": [
                "### Calculate Cosine Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd8427af",
            "metadata": {},
            "outputs": [],
            "source": [
                "similar_rows = Matcher.get_query_answers(lsa_matrix,matrix,dataset_keys,0.9)\n",
                "\n",
                "for row in similar_rows.items():\n",
                "    print(row)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "17659ba6",
            "metadata": {},
            "source": [
                "## Evaluation Queries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db528201",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/testing/queries.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c718b9f1",
            "metadata": {},
            "source": [
                "### Lotte queries loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d15389a",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/queries.csv\",delimiter=\"\\t\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63e5cbd5",
            "metadata": {},
            "source": [
                "### Text Processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9dcc6d1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from multiprocessing import Pool\n",
                "\n",
                "for key in queries.keys():\n",
                "    # queries[key] = [WordCleaner.get_unified_synonym(word) for word in queries[key]]\n",
                "    queries[key] = WordCleaner.remove_stop_words(queries[key])\n",
                "#     queries[key] = WordCleaner.process_capital_punctuation(queries[key])\n",
                "# with Pool() as p:\n",
                "#     for row in tqdm(queries):\n",
                "#         queries[row] = p.map(WordCleaner.get_unified_synonym_2, queries[row])\n",
                "#     queries[key] = WordCleaner.stem(queries[key], \"Snowball\")\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.lemmatize(queries[key])\n",
                "for key in queries.keys():\n",
                "    queries[key] = WordCleaner.remove_single_letters(queries[key])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0320dc0f",
            "metadata": {},
            "source": [
                "### Calculate TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "260722c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries_matrices = {}\n",
                "for key in tqdm(queries.keys()):\n",
                "    queries_matrices[key] = Indexer.calculate_doc_tf_idf([\" \".join(queries[key])],vectorizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7d46388",
            "metadata": {},
            "source": [
                "### Calculate Cosine Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "562157b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "queriesAnswers = {}\n",
                "for key in tqdm(queries_matrices.keys()):\n",
                "    queriesAnswers[key] = Matcher.get_query_answers(tfidf_matrix,queries_matrices[key],dataset_keys,0.35)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36504471",
            "metadata": {},
            "outputs": [],
            "source": [
                "queriesAnswers = {}\n",
                "for key in tqdm(queries_matrices.keys()):\n",
                "    queriesAnswers[key] = Matcher.get_query_answers(tfidf_matrix,queries_matrices[key],dataset_keys,0.35)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1edcaba6",
            "metadata": {},
            "source": [
                "# Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "11d3dcab",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirNRML35.run\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71bc01c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirRMLN35.run\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b968cff3",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"TwikirRML35.run\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3311fe2",
            "metadata": {},
            "source": [
                "# Write To Files"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "731ad4ac",
            "metadata": {},
            "source": [
                "## Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13c2eea2",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_dataset_to_file(\"wikirRML.csv\",datasets[-1])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d8a34c7e",
            "metadata": {},
            "source": [
                "## Run File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d525af42",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_runfile_to_file(\"TwikirRMLN35.run\",queries,queriesAnswers,max_relevance=2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7bdbe7ab",
            "metadata": {},
            "source": [
                "## Model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3094d4e4",
            "metadata": {},
            "source": [
                "### Write"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "379b09d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "FileManager.write_model_to_drive(\"wikir_RMLN\",vectorizer, dataset_keys, tfidf_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34d03817",
            "metadata": {},
            "source": [
                "### Read"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8da4498",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer, dataset_keys, tfidf_matrix = FileManager.load_model_from_drive(\"lotteLSA\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b6b2e89f",
            "metadata": {},
            "source": [
                "# Test Bert"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f59aeb70",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import BertTokenizer, BertModel\n",
                "from tqdm import tqdm\n",
                "\n",
                "model = BertModel.from_pretrained('./model',ignore_mismatched_sizes=True)\n",
                "tokenizer = BertTokenizer.from_pretrained('./model',ignore_mismatched_sizes=True)\n",
                "dataset = datasets[-1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a95e9940",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"wikir/testing/queries.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "974869b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenized_documents = {doc_id: tokenizer(' '.join(words), return_tensors='pt') for doc_id, words in tqdm(dataset.items())}\n",
                "\n",
                "tokenized_queries = {query_id: tokenizer(' '.join(words), return_tensors='pt') for query_id, words in queries.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e52bf639",
            "metadata": {},
            "outputs": [],
            "source": [
                "query_vectors = {query_id: model(**words)[0][0][0] for query_id, words in tokenized_queries.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3eca9965",
            "metadata": {},
            "outputs": [],
            "source": [
                "document_vectors = {doc_id: model(**words)[0][0][0] for doc_id, words in tqdm(tokenized_documents.items())}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4d65a2f4",
            "metadata": {},
            "outputs": [],
            "source": [
                "document_vector = {'1781133': model(**tokenized_documents[\"1781133\"])[0][0][0]}\n",
                "document_vector"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dfffbad2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "device = torch.device('cuda')\n",
                "model = model.to(device)\n",
                "\n",
                "document_vectors = {}\n",
                "batch_size = 512  # Adjust this based on your VRAM availability\n",
                "\n",
                "doc_ids = list(tokenized_documents.keys())\n",
                "doc_batches = [doc_ids[i:i + batch_size] for i in range(0, len(doc_ids), batch_size)]\n",
                "\n",
                "for i,batch in enumerate(doc_batches):\n",
                "    print(f\"Processing batch {i} out of {len(doc_batches)}\")\n",
                "    batch_dict = {doc_id: tokenized_documents[doc_id] for doc_id in batch}\n",
                "    with torch.no_grad():\n",
                "        for doc_id, words in tqdm(batch_dict.items()):\n",
                "            document_vectors[doc_id] = model(**words.to(device))[0][0][0]\n",
                "            torch.cuda.empty_cache()  # Free up unused memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e6f0f6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "b0 = np.load(\"batch0.npy\",allow_pickle=True)\n",
                "b1 = np.load(\"batch1.npy\",allow_pickle=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e69c9ad9",
            "metadata": {},
            "outputs": [],
            "source": [
                "b0 | b1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a449471a",
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
                "\n",
                "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                "embeddings = model.encode(sentences)\n",
                "print(embeddings)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1327f932",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "device = torch.device('cuda')\n",
                "model = model.to(device)\n",
                "\n",
                "document_vectors = {}\n",
                "batch_size = 512  # Adjust this based on your VRAM availability\n",
                "\n",
                "doc_ids = list(tokenized_documents.keys())\n",
                "doc_batches = [doc_ids[i:i + batch_size] for i in range(0, len(doc_ids), batch_size)]\n",
                "\n",
                "for i, batch in enumerate(doc_batches):\n",
                "    # document_vectors = {}\n",
                "    print(f\"Processing batch {i} out of {len(doc_batches)}\")\n",
                "    batch_dict = {doc_id: tokenized_documents[doc_id] for doc_id in batch}\n",
                "    with torch.no_grad():\n",
                "        for doc_id, words in tqdm(batch_dict.items()):\n",
                "            document_vectors[doc_id] = model(**words)[0][0][0]\n",
                "    # np.save(f\"batch{i}\",document_vectors)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7edc2f0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert BERT embeddings to 2D numpy arrays\n",
                "document_vectors_np = {doc_id: doc_vector.detach().cpu().numpy().reshape(1, -1) for doc_id, doc_vector in document_vectors.items()}\n",
                "query_vectors_np = {query_id: query_vector.detach().cpu().numpy().reshape(1, -1) for query_id, query_vector in query_vectors.items()}\n",
                "\n",
                "# Create corpus_matrix and query_matrix\n",
                "corpus_matrix = np.vstack(list(document_vectors_np.values()))\n",
                "query_matrix = np.vstack(list(query_vectors_np.values()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f9cd1f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries_answers = {}\n",
                "for key in queries.keys():\n",
                "    queries_answers[key] = Matcher.get_query_answers(corpus_matrix, query_vectors_np[key], dataset_keys, 0.85)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "89c95f3f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write to run file\n",
                "FileManager.write_runfile_to_file('bert85.run', queries, queries_answers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write to run file\n",
                "FileManager.write_runfile_to_file('bert85.run', queries, queries_answers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "216e37fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"wikir/testing/qrels\",\"bert85.run\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7717c457",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manual Query\n",
                "query_words = [\"the film was directed by lloyd ingraham who co wrote the film adaption with david kirkland it stars betty ross clarke earl schenck and wade boteler the film is today considered lost the screenplay was based on the eponymous novel by myrtle reed who also wrote cookbooks under the pen name olivia green and committed suicide in 1911 harlan carr and his wife are left 600 and a country home called the jack o lantern in new england by his uncle s will the will provides that a future legacy will come to him if he lives in the estate for six months carr and his wife take up residence in the home where all kinds of ghostly events take place to frighten them out a group of guests arrive who were all disinherited relatives to his uncle and they try to take over they quickly make life miserable for the couple who tolerate them and their unpleasantness only out of fear of losing the legacy at last harlan loses all patience and orders them from the house then unexpectedly the family lawyer informs the carrs that having done exactly as their uncle had wished they would they will be\"]\n",
                "\n",
                "# Tokenize and convert your query to IDs\n",
                "tokenized_query = tokenizer(' '.join(query_words), return_tensors='pt')\n",
                "\n",
                "# Move indexed_query to the correct device and calculate the query vector\n",
                "query_vector = model(**tokenized_query.to(device))[0][0][0]\n",
                "\n",
                "# Move query_vector to the CPU, detach it from the computation graph, and convert it to a numpy array\n",
                "query_matrix = query_vector.detach().cpu().numpy().reshape(1, -1)\n",
                "\n",
                "# Use your function to get the most similar documents\n",
                "similar_docs = Matcher.get_query_answers(corpus_matrix, query_matrix, dataset_keys, 0.85)\n",
                "\n",
                "# Print the IDs of the top 5 most similar documents\n",
                "for i, (doc_id, score) in enumerate(list(similar_docs.items())[:10]):\n",
                "    print(f\"Rank {i+1}, Document ID: {doc_id}, Similarity Score: {score}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aadbb0b4",
            "metadata": {},
            "source": [
                "# Test Sentence-Transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02d09531",
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8b87f3d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = datasets[-1]\n",
                "embeddings = []\n",
                "keys = list(dataset.keys())\n",
                "values = list(dataset.values())\n",
                "batch_size = 10000  # Adjust this value based on your memory capacity\n",
                "\n",
                "for i in tqdm(range(0, len(keys), batch_size)):\n",
                "    batch_keys = keys[i:i+batch_size]\n",
                "    batch_values = values[i:i+batch_size]\n",
                "    batch_embeddings = model.encode(batch_values)\n",
                "    for embedding in batch_embeddings:\n",
                "        embeddings.append(embedding)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "01007786",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy\n",
                "numpy.save(\"lotte_sent_embed\",embeddings)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c559fd2",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = FileManager.csv_to_dict(\"lotte/queries.tsv\",delimiter=\"\\t\") | FileManager.csv_to_dict(\"lotte/queries2.tsv\",delimiter=\"\\t\")\n",
                "embeddingsQ = {}\n",
                "for key in tqdm(queries.keys()):\n",
                "    embeddingsQ[key] = model.encode(\" \".join(queries[key]))\n",
                "    embeddingsQ[key] = embeddingsQ[key].reshape(1,-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5abae459",
            "metadata": {},
            "outputs": [],
            "source": [
                "queries_answers = {}\n",
                "corpus_matrix = np.array(embeddings)\n",
                "for key in tqdm(queries.keys()):\n",
                "    queries_answers[key] = Matcher.get_query_answers(corpus_matrix, embeddingsQ[key], dataset_keys, 0.35)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d86b49ac",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write to run file\n",
                "FileManager.write_runfile_to_file('lottesent65.run', queries, queries_answers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9266208a",
            "metadata": {},
            "outputs": [],
            "source": [
                "Evaluater.evaluate(\"lotte/qrels\",\"lottesent65.run\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
