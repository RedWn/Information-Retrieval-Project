{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ab75af",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import FileManager\n",
    "import WordCleaner\n",
    "import Indexer\n",
    "import Matcher\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c2740",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d69ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FileManager.csvToDict('testing.csv')\n",
    "datasets = [dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621f9e3",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "stemmed_dataset = {}\n",
    "for row in dataset:\n",
    "    stemmed_words = WordCleaner.stem(dataset[row], 'porter')\n",
    "    stemmed_dataset[row] = stemmed_words\n",
    "datasets.append(stemmed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7a58d",
   "metadata": {},
   "source": [
    "## OR Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "lemmad_dataset = {}\n",
    "for row in dataset:\n",
    "    stemmed_words = WordCleaner.lemmatize(dataset[row])\n",
    "    lemmad_dataset[row] = stemmed_words\n",
    "datasets.append(lemmad_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae375500",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65723f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "filtered_dataset = {}\n",
    "for key in dataset:\n",
    "    filtered_dataset[key] = WordCleaner.removeStopWords(dataset[key])\n",
    "datasets.append(filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995bcb4",
   "metadata": {},
   "source": [
    "## Creating the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = Indexer.getInvertedIndex(datasets[-1])\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f368df",
   "metadata": {},
   "source": [
    "## Calculating tf-idf for the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b60ed8",
   "metadata": {},
   "source": [
    "### using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tfidf_matrix, df) = Indexer.calculateTF_IDF(datasets[-1])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ba333",
   "metadata": {},
   "source": [
    "### Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "(tfidf_matrix, df) = Indexer.calculateManualTF_IDF(datasets[-1])\n",
    "\n",
    "ans = csr_matrix(arg1=tfidf_matrix, dtype=np.float64)\n",
    "ans\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152773d9",
   "metadata": {},
   "source": [
    "## Calculate Cosine Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51477a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_docs = Matcher.calcCosSimWithCorpus(df,df.iloc[8])\n",
    "\n",
    "# Print the sorted related documents\n",
    "print(\"Related Docs (similarity > 0.5):\")\n",
    "for doc, sim in related_docs:\n",
    "    print(f\"Doc {doc}: Similarity = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdd8aa",
   "metadata": {},
   "source": [
    "## Query Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17659ba6",
   "metadata": {},
   "source": [
    "### Enter the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "query = 'when was yanni born ?'\n",
    "query = word_tokenize(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11898f4",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = WordCleaner.removeStopWords(query)\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef04e2d",
   "metadata": {},
   "source": [
    "### Stem or Lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a335552",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = WordCleaner.stem(query, 'porter')\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16696076",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859dc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "(query_tfidf_matrix, qdf) = Indexer.calculateDocTF_IDF(datasets[-1],query)\n",
    "\n",
    "qdf\n",
    "# query_tfidf_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e6073",
   "metadata": {},
   "source": [
    "### Calculate Cos Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8427af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# related_docs = Matcher.calcCosSimWithCorpus(df,query_tfidf_matrix)\n",
    "\n",
    "A_array = np.array(qdf.iloc[0])\n",
    "# A_array = np.array(df.iloc[8])\n",
    "related_docs = []\n",
    "for index, row in df.iterrows():\n",
    "    B_array = np.array(row)  # Access the row data (vector B)\n",
    "    similarity = cosine_similarity([A_array], [B_array])[0][0]\n",
    "    print(f\"Cosine Similarity between doc 1 and doc {index}: {similarity:.4f}\")\n",
    "    if similarity > 0.5:\n",
    "        related_docs.append((index, similarity))\n",
    "# Sort related_docs by similarity (highest to lowest)\n",
    "related_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "# return related_docs\n",
    "\n",
    "# Print the sorted related documents\n",
    "print(\"Related Docs (similarity > 0.5):\")\n",
    "for doc, sim in related_docs:\n",
    "    print(f\"Doc {doc}: Similarity = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3311fe2",
   "metadata": {},
   "source": [
    "## Write To File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer, file = FileManager.openCSVWriter('stemmed.csv',['id','text'])\n",
    "for key in datasets[-1]:\n",
    "    file_writer.writerow({'id': key, 'text': datasets[-1][key]})\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
