{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ab75af",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118854ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# !pip install torch==2.2.2\n",
    "\n",
    "# %pip install country_converter\n",
    "\n",
    "# %pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# %pip install gensim\n",
    "\n",
    "# %pip install roman\n",
    "# %pip install re\n",
    "# %pip install tqdm\n",
    "# %pip install \"torch-2.2.2+cu121-cp311-cp311-win_amd64.whl\"\n",
    "# %pip install bert-extractive-summarizer\n",
    "# %pip install geocoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python import FileManager\n",
    "from python import WordCleaner\n",
    "from python import Indexer\n",
    "from python import Matcher\n",
    "from python import Evaluater\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b2947",
   "metadata": {},
   "source": [
    "# Dataset Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c2740",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d69ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "dataset = FileManager.csv_to_dict(\"wikir/documents.csv\")\n",
    "datasets = [dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15979398",
   "metadata": {},
   "source": [
    "### The Ultimate Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3281041",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "dataset = {}\n",
    "for i in range(0,4):\n",
    "    dataset = dataset | FileManager.csv_to_dict(f\"wikir/R{i}.csv\")\n",
    "datasets = [dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae375500",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65723f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "filtered_dataset = {}\n",
    "for key in dataset:\n",
    "    filtered_dataset[key] = WordCleaner.remove_stop_words(dataset[key])\n",
    "datasets.append(filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7120a1",
   "metadata": {},
   "source": [
    "## Remove single letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "no_singles_dataset = {}\n",
    "for key in dataset:\n",
    "    no_singles_dataset[key] = WordCleaner.remove_single_letters(dataset[key])\n",
    "datasets.append(no_singles_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621f9e3",
   "metadata": {},
   "source": [
    "## Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "stemmed_dataset = {}\n",
    "for row in dataset:\n",
    "    stemmed_dataset[row] = WordCleaner.stem(dataset[row], \"Snowball\")\n",
    "datasets.append(stemmed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7a58d",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "lemmad_dataset = {}\n",
    "for row in dataset:\n",
    "    lemmad_dataset[row] = WordCleaner.lemmatize(dataset[row])\n",
    "datasets.append(lemmad_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0bcf1",
   "metadata": {},
   "source": [
    "## Synonym Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = datasets[-1]\n",
    "mapped_2 = {}\n",
    "\n",
    "# Create a pool of workers\n",
    "with Pool() as p:\n",
    "    # Wrap your iterator (dataset) with tqdm for a progress bar\n",
    "    for row in tqdm(dataset):\n",
    "        # Apply the function to each word in the row in parallel\n",
    "        mapped_2[row] = p.map(WordCleaner.get_unified_synonym_2, dataset[row])\n",
    "datasets.append(mapped_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f368df",
   "metadata": {},
   "source": [
    "## Calculating tf-idf for the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b60ed8",
   "metadata": {},
   "source": [
    "### using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_matrix = Indexer.calculate_tf_idf(datasets[-1], vectorizer)\n",
    "dataset_keys = list(datasets[-1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdd8aa",
   "metadata": {},
   "source": [
    "# Query Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb756f25",
   "metadata": {},
   "source": [
    "## Manual Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad178",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"wikipedia\"\n",
    "query = word_tokenize(query)\n",
    "query = WordCleaner.remove_stop_words(query)\n",
    "query = WordCleaner.remove_single_letters(query)\n",
    "# query = WordCleaner.stem(query, 'Snowball')\n",
    "query = [WordCleaner.get_unified_synonym_2(word) for word in query]\n",
    "query = WordCleaner.lemmatize(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079d6ac",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859dc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = Indexer.calculate_doc_tf_idf([\" \".join(query)],vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08b71d",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8427af",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_rows = Matcher.get_query_answers(tfidf_matrix, matrix, dataset_keys, 0.35)\n",
    "\n",
    "# Sort the items in the dictionary by value (i.e., rating) in descending order\n",
    "sorted_rows = sorted(similar_rows.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "for row in sorted_rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17659ba6",
   "metadata": {},
   "source": [
    "## Evaluation Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db528201",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = FileManager.csv_to_dict(\"wikir/test/queries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in queries.keys():\n",
    "    queries[key] = WordCleaner.remove_stop_words(queries[key])\n",
    "for key in queries.keys():\n",
    "    queries[key] = WordCleaner.remove_single_letters(queries[key])\n",
    "for key in queries.keys():\n",
    "    queries[key] = [WordCleaner.get_unified_synonym_2(word) for word in queries[key]]\n",
    "# for key in queries.keys():\n",
    "#     queries[key] = WordCleaner.stem(queries[key], \"Snowball\")\n",
    "for key in queries.keys():\n",
    "    queries[key] = WordCleaner.lemmatize(queries[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320dc0f",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3903f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_matrices = {}\n",
    "for key in queries.keys():\n",
    "    queries_matrices[key] = Indexer.calculate_doc_tf_idf([\" \".join(queries[key])],vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d46388",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562157b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queriesAnswers = {}\n",
    "for key in queries.keys():\n",
    "    queriesAnswers[key] = Matcher.get_query_answers(tfidf_matrix,queries_matrices[key],dataset_keys,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcaba6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_RRM2L_01.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ee075",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"testrun_RRL_01.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_RRML_01.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544defa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_RML_01.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_bert_testing.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71addda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_embedding_5_epoch_1_tfidf_07.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_doc2vec_epoch_1_07.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_doc2vec_epoch_1_05.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aacf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_doc2vec_epoch_9_065.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf78109",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_embedding_5_epoch_15_065.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91bf3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_embedding_5_epoch_15_06.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_embedding_5_epoch_22_06.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/test/qrels\",\"Tbert_docs_08.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"bert_docs_08.run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3311fe2",
   "metadata": {},
   "source": [
    "# Write To Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ad4ac",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileManager.write_dataset_to_file(\"wikir/RRM2L.csv\",datasets[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a34c7e",
   "metadata": {},
   "source": [
    "## Run File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileManager.write_runfile_to_file(\"test runs/testrun_RRM2L_01.run\",queries,queriesAnswers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbe7ab",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094d4e4",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileManager.write_model_to_drive(\"models/Model_RRM2L\",vectorizer, dataset_keys, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d03817",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da4498",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, svd, dataset_keys, tfidf_matrix = FileManager.load_model_from_drive(\"models/Model_RM_NEW_L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a15f9c",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e64c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "dataset = datasets[-1]\n",
    "# Convert to a list of tokenized documents\n",
    "tokenized_documents = dataset.values()\n",
    "model = Word2Vec(sentences=tokenized_documents,\n",
    "                          vector_size=250,  # Dimensionality of the word vectors (100 is Good for a medium-sized dataset)\n",
    "                          window=5,         # Maximum distance between the current and predicted word within a sentence ( 5 Balances local and broader context)\n",
    "                          sg=1,             # Skip-Gram model (1 for Skip-Gram (can capture complex patterns), 0 for CBOW)\n",
    "                          min_count=1,      # Ignores all words with a total frequency lower than this (2 is Low enough to not lose infrequent words)\n",
    "                          workers = 10\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"embedding_5_epoch_22.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70650f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "dataset = datasets[-1]\n",
    "# Convert to a list of tokenized documents\n",
    "tokenized_documents = dataset.values()\n",
    "model = Word2Vec.load(\"embedding_5_epoch_1.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train(tokenized_documents, total_examples=len(tokenized_documents), epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4843323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "document_vectors = []\n",
    "for doc in tokenized_documents:\n",
    "    # Filter out tokens not in the model's vocabulary\n",
    "    valid_tokens = [token for token in doc if token in model.wv]\n",
    "    # Calculate the average vector for each document\n",
    "    if valid_tokens:  # Check if there are any valid tokens\n",
    "        doc_vector = np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "        document_vectors.append(doc_vector)\n",
    "    else:\n",
    "        # Handle documents with no valid tokens (e.g., empty documents)\n",
    "        document_vectors.append(np.zeros(model.vector_size))\n",
    "\n",
    "# Convert to a 2D array\n",
    "document_vectors = np.array(document_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f15ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_matrices = {}\n",
    "for key, query_tokens in queries.items():\n",
    "    # Filter out tokens not in the model's vocabulary\n",
    "    valid_tokens = [token for token in query_tokens if token in model.wv]\n",
    "\n",
    "    # Calculate the average vector for each query\n",
    "    if valid_tokens:\n",
    "        query_vector = np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "        queries_matrices[key] = query_vector\n",
    "    else:\n",
    "        # Handle queries with no valid tokens\n",
    "        print(\"Query with no valid tokens: \" + key)\n",
    "        queries_matrices[key] = np.zeros(model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "corpus_matrix_sparse = sparse.csr_matrix(document_vectors)\n",
    "queries_answers_embedded = {}\n",
    "\n",
    "for key in queries.keys():\n",
    "    # Reshape the query vector to 2D\n",
    "    query_vector_2d = queries_matrices[key].reshape(1, -1)\n",
    "    # Calculate answers for one query at a time\n",
    "    queries_answers_embedded[key] = Matcher.get_query_answers_optimized(corpus_matrix_sparse, query_vector_2d, dataset_keys, 0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d525019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to run file\n",
    "FileManager.write_runfile_to_file('testrun_embedding_5_epoch_22_06.run', queries, queries_answers_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual query\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a query vector for \"tokyo disney resort\"\n",
    "query_words = [\"apple\"]\n",
    "valid_vectors = [model.wv[word] for word in query_words if word in model.wv]\n",
    "\n",
    "# Check if there are valid vectors to avoid nan issues\n",
    "if valid_vectors:\n",
    "    query_vector = np.mean(valid_vectors, axis=0).reshape(1, -1)\n",
    "    # Compute cosine similarity between query and document vectors\n",
    "    similarity_scores = cosine_similarity(query_vector, document_vectors)\n",
    "\n",
    "    # Rank documents based on similarity scores\n",
    "    sorted_docs = sorted(enumerate(similarity_scores[0]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the ranked documents\n",
    "    for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "        print(f\"Rank {rank}: Document {doc_id + 2} (Similarity Score = {score:.4f})\")\n",
    "else:\n",
    "    print(\"None of the query words were found in the model's vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e2a74",
   "metadata": {},
   "source": [
    "## Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import platform\n",
    "user_os = str(platform.system())\n",
    "\n",
    "history_query = [\"tech\"]\n",
    "query = [\"apple\"]\n",
    "\n",
    "vector_1 = [model.wv[word] for word in query if word in model.wv]\n",
    "vector_2 = [model.wv[word] for word in history_query if word in model.wv]\n",
    "\n",
    "# Check if there are valid vectors to avoid nan issues\n",
    "if vector_1:\n",
    "    query_vector_1 = np.mean(vector_1, axis=0).reshape(1, -1)\n",
    "    query_vector_2 = np.mean(vector_2, axis=0).reshape(1, -1)\n",
    "\n",
    "    # Assuming query_vector_1 and query_vector_2 are already defined as 2D arrays\n",
    "    weighted_vector = 0.75 * query_vector_1 + 0.25 * query_vector_2\n",
    "    # Compute cosine similarity between query and document vectors\n",
    "    similarity_scores = cosine_similarity(weighted_vector, document_vectors)\n",
    "    # Rank documents based on similarity scores\n",
    "    sorted_docs = sorted(enumerate(similarity_scores[0]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the ranked documents\n",
    "    for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "        print(f\"Rank {rank}: Document in row {doc_id + 2} (Similarity Score = {score:.4f})\")\n",
    "else:\n",
    "    print(\"None of the query words were found in the model's vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b50fe7",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d60099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.wv['malaysia'])\n",
    "# print(model.wv.similarity('1st', 'First'))\n",
    "\n",
    "print(model.wv.similarity('world', 'war'))\n",
    "print(model.wv.similarity('good', 'best'))\n",
    "print(model.wv.most_similar('war'))\n",
    "print(model.wv.most_similar(positive=['king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e3bb7",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41071cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words is None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.index_to_key), sample)\n",
    "        else:\n",
    "            words = list(model.wv.index_to_key)\n",
    "        \n",
    "    word_vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "    # Determine the appropriate number of components (up to min(n_samples, n_features))\n",
    "    n_components = min(word_vectors.shape[0], word_vectors.shape[1])\n",
    "\n",
    "    if n_components > 1:\n",
    "        twodim = PCA(n_components=n_components).fit_transform(word_vectors)[:, :2]\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='r')\n",
    "        for word, (x, y) in zip(words, twodim):\n",
    "            plt.text(x + 0.05, y + 0.05, word)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Insufficient data for PCA visualization.\")\n",
    "\n",
    "# Example usage:\n",
    "display_pca_scatterplot(model, ['battalion','world', 'war', 'good', 'best', 'state', 'government', 'university', 'college', 'germany', 'german', '12', 'twelve'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(document_vectors)\n",
    "\n",
    "plt.figure(1, figsize=(30, 20),)\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1],s=100, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(list(document_vectors))\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(30, 20))\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], alpha=0.2, s=100)\n",
    "\n",
    "# Optionally, add labels to the points\n",
    "for i, doc_id in enumerate(document_vectors):\n",
    "    plt.annotate(doc_id, (x_pca[i, 0], x_pca[i, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For when we do clustring, assign the cluter to each doc and visualize here\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'document_topics' is a list of topics for each document\n",
    "document_topics = ...\n",
    "\n",
    "# Create a colormap for the topics\n",
    "cmap = plt.get_cmap('tab10')\n",
    "colors = [cmap(i) for i in range(len(set(document_topics)))]\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(list(document_vectors.values()))\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(30, 20))\n",
    "for i, topic in enumerate(set(document_topics)):\n",
    "    plt.scatter(x_pca[document_topics == topic, 0], x_pca[document_topics == topic, 1], c=[colors[i]], label=topic, alpha=0.2, s=100)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0d2f8",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238aea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "dataset = datasets[-1]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=words, tags=[doc_id]) for doc_id, words in dataset.items()]\n",
    "\n",
    "# Build and train the model\n",
    "model = Doc2Vec(vector_size=200, window=5, min_count=1, workers=8, epochs=1)\n",
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148de76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_doc2vec_epoch_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26674b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"my_doc2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document vectors\n",
    "document_vectors = {doc_id: model.dv[doc_id] for doc_id in dataset_keys}\n",
    "\n",
    "# Infer a vector for each query\n",
    "query_vectors = {query_id: model.infer_vector(words) for query_id, words in queries.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# Convert document_vectors to a sparse matrix\n",
    "document_vectors_matrix = sparse.csr_matrix(list(document_vectors.values()))\n",
    "\n",
    "# Use your get_query_answers function to retrieve relevant documents\n",
    "queries_answers = {}\n",
    "for query_id, query_vector in query_vectors.items():\n",
    "    # Reshape the query vector to 2D\n",
    "    query_vector_2d = query_vector.reshape(1, -1)\n",
    "    queries_answers[query_id] = Matcher.get_query_answers(document_vectors_matrix, query_vector_2d, dataset_keys, 0.65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bbd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to run file\n",
    "FileManager.write_runfile_to_file('testrun_doc2vec_epoch_9_065.run', queries, queries_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b384129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual query\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_words = [\"wikipedia\"]\n",
    "query_vector = model.infer_vector(query_words)\n",
    "\n",
    "# Reshape the query vector to 2D\n",
    "query_vector_2d = query_vector.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query vector and document vectors\n",
    "similarity_scores = cosine_similarity(query_vector_2d, list(document_vectors.values()))\n",
    "# Rank documents based on similarity scores\n",
    "sorted_docs = sorted(enumerate(similarity_scores[0]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the ranked documents\n",
    "for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_id + 2} (Similarity Score = {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa37a0",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c56dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcebb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = datasets[-1]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize your documents\n",
    "tokenized_documents = {doc_id: tokenizer.tokenize(' '.join(words)) for doc_id, words in tqdm(dataset.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your queries\n",
    "tokenized_queries = {query_id: tokenizer.tokenize(' '.join(words)) for query_id, words in queries.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to IDs\n",
    "indexed_documents = {doc_id: tokenizer.convert_tokens_to_ids(words) for doc_id, words in tokenized_documents.items()}\n",
    "indexed_queries = {query_id: tokenizer.convert_tokens_to_ids(words) for query_id, words in tokenized_queries.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "scaler = GradScaler()  # Initialize GradScaler for mixed precision training\n",
    "document_vectors = {}\n",
    "\n",
    "doc_ids = list(indexed_documents.keys())\n",
    "\n",
    "batch_size = 500\n",
    "doc_batches = [doc_ids[i:i + batch_size] for i in range(0, len(doc_ids), batch_size)]\n",
    "\n",
    "counter = 1\n",
    "for batch in doc_batches:\n",
    "    print(f\"Processing batch {counter} out of {len(doc_batches)}\")\n",
    "    # Calculate document vectors for the current batch\n",
    "    batch_words = [indexed_documents[doc_id] for doc_id in batch]\n",
    "    max_length = min(512, max(len(words) for words in batch_words))  # Find the maximum length, but do not exceed 512\n",
    "    batch_words_padded = [words[:max_length] + [0]*(max_length-len(words)) for words in batch_words]  # Pad or truncate all sequences to the maximum length\n",
    "    batch_words_tensor = torch.tensor(batch_words_padded).to(device)\n",
    "    with torch.no_grad():\n",
    "        with autocast():  # Enable autocasting for mixed precision\n",
    "            batch_vectors = model(batch_words_tensor)[0][:, 0, :]\n",
    "    for i, doc_id in enumerate(batch):\n",
    "        document_vectors[doc_id] = batch_vectors[i].cpu()  # Move the vectors back to CPU\n",
    "    torch.cuda.empty_cache()\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27556716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# dftemp = pd.DataFrame(document_vectors)\n",
    "document_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a37027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate query vectors\n",
    "query_vectors = {}\n",
    "for query_id, words in indexed_queries.items():\n",
    "    # Pad or truncate to the same max_length as used for documents\n",
    "    words = words[:max_length] + [0]*(max_length-len(words))\n",
    "    words_tensor = torch.tensor([words]).to(device)\n",
    "    with torch.no_grad():\n",
    "        with autocast():  # Enable autocasting for mixed precision\n",
    "            # Check out the line under\n",
    "            query_vectors[query_id] = model(words_tensor)[0][0][0].cpu()  # Move the vector back to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors['1781133'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d912084",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BERT embeddings to 2D numpy arrays\n",
    "document_vectors_np = {doc_id: doc_vector.detach().cpu().numpy().reshape(1, -1) for doc_id, doc_vector in document_vectors.items()}\n",
    "query_vectors_np = {query_id: query_vector.detach().cpu().numpy().reshape(1, -1) for query_id, query_vector in query_vectors.items()}\n",
    "\n",
    "# Create corpus_matrix and query_matrix\n",
    "corpus_matrix = np.vstack(list(document_vectors_np.values()))\n",
    "query_matrix = np.vstack(list(query_vectors_np.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(document_vectors.keys())[548]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82483c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = cosine_similarity(query_matrix4, query_matrix5).reshape(-1)\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe787b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(similarity_matrix > .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f609e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d36660",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(similarity_matrix > 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c609cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "queries_answers = {}\n",
    "for key in tqdm(queries.keys()):\n",
    "    queries_answers[key] = Matcher.get_query_answers(corpus_matrix, query_vectors_np[key], dataset_keys, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21824b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to run file\n",
    "FileManager.write_runfile_to_file('Tbert_docs_08III.run', queries, queries_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/test/qrels\",\"Tbert_docs_08II.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/test/qrels\",\"Tbert_docs_08III.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac47582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Query\n",
    "query_words = [\"a graduate of franklin marshall college where he earned a degree in teaching and spent four years on several of its sports teams bridenbaugh coached football at several places in his home state of pennsylvania prior to being selected as the head coach of the geneva college golden tornadoes in 1917 he left geneva in 1922 with a 23 12 5 record and took a job with new castle junior senior high school as a mathematics teacher and head football basketball and track and field coach he did not lose a football game in his first two years marking the first of eleven undefeated seasons and over the course of 33 years won seven league titles in the sport leaving in 1955 with a 265 65 25 record he continued to work as an assistant football coach at grove city college until 1964 and was inducted into several regional halls of fame he died in june 1990 at the age of 100 bridenbaugh was born on may 1 1890 in martinsburg pennsylvania one of 14 children of professor phillip howard bridenbaugh an educator and academic administrator and catherine oelling he attended altoona area high school in altoona pennsylvania for one\"]\n",
    "# Tokenize and convert your query to IDs\n",
    "tokenized_query = tokenizer.tokenize(' '.join(query_words))\n",
    "indexed_query = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
    "\n",
    "# Pad or truncate to the same max_length as used for documents\n",
    "indexed_query = indexed_query[:max_length] + [0]*(max_length-len(indexed_query))\n",
    "\n",
    "# Move indexed_query to the correct device and calculate the query vector\n",
    "indexed_query_tensor = torch.tensor([indexed_query]).to(device)\n",
    "with torch.no_grad():\n",
    "    with autocast():  # Enable autocasting for mixed precision\n",
    "        query_vector = model(indexed_query_tensor)[0][0][0]\n",
    "\n",
    "# Move query_vector to the CPU, detach it from the computation graph, and convert it to a numpy array\n",
    "query_matrix = query_vector.cpu().numpy().reshape(1, -1)\n",
    "\n",
    "# Use your function to get the most similar documents\n",
    "similar_docs = Matcher.get_query_answers(corpus_matrix, query_matrix,dataset_keys, 0.85)\n",
    "\n",
    "# Print the IDs of the top 5 most similar documents\n",
    "for i, (doc_id, score) in enumerate(list(similar_docs.items())[:20]):\n",
    "    print(f\"Rank {i+1}, Document ID: {doc_id}, Similarity Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e32660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming document_vectors is your dictionary of document vectors\n",
    "doc_id = list(document_vectors.keys())[0]  # Get the ID of the first document\n",
    "vector = document_vectors[doc_id]  # Get the vector for the first document\n",
    "print(\"Vector dimensions:\", np.array(vector).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "from transformers import BertModel\n",
    "\n",
    "# Input text to be summarized\n",
    "input_text = \"As you move from left to right across a period, the number of protons in the nucleus increases. The electrons are thus attracted to the nucleus more strongly, and the atomic radius is smaller (this attraction is much stronger than the relatively weak repulsion between electrons). As you move down a column, there are more protons, but there are also more complete energy levels below the valence electrons. These lower energy levels shield the valence electrons from the attractive effects of the atom's nucleus, so the atomic radius gets larger.\"\n",
    "\n",
    "# # Load pre-trained model (weights)\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# # Create a BERT extractive summarizer\n",
    "# summarizer = Summarizer(custom_model=model)\n",
    "\n",
    "# Create a BERT extractive summarizer\n",
    "summarizer = Summarizer(model = 'bert-base-uncased')\n",
    "\n",
    "\n",
    "# Generate the summary\n",
    "summary = summarizer(input_text, min_length=30, max_length=300)  # You can adjust the min_length and max_length parameters\n",
    "\n",
    "# Output the summary\n",
    "print(\"Original Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder\n",
    "g = geocoder.ip('me')\n",
    "country = str(g.country)\n",
    "city = str(g.city)\n",
    "if g.country is None:\n",
    "    country = \"syria\"\n",
    "    city = \"damascus\"\n",
    "print(country)\n",
    "print(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"bert_docs_vecs_0\",document_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
