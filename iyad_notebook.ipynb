{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ab75af",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118854ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# !pip install torch==2.2.2\n",
    "\n",
    "# %pip install country_converter\n",
    "\n",
    "# %pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# %pip install gensim\n",
    "\n",
    "# %pip install roman\n",
    "# %pip install re\n",
    "%pip install tqdm\n",
    "%pip install \"torch-2.2.2+cu121-cp311-cp311-win_amd64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python import FileManager\n",
    "from python import WordCleaner\n",
    "from python import Indexer\n",
    "from python import Matcher\n",
    "from python import Evaluater\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b2947",
   "metadata": {},
   "source": [
    "# Dataset Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c2740",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d69ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "dataset = FileManager.csv_to_dict(\"wikir/RR.csv\")\n",
    "datasets = [dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15979398",
   "metadata": {},
   "source": [
    "### The Ultimate Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3281041",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "dataset = {}\n",
    "for i in range(0,4):\n",
    "    dataset = dataset | FileManager.csv_to_dict(f\"wikir/R{i}.csv\")\n",
    "datasets = [dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae375500",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65723f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "filtered_dataset = {}\n",
    "for key in dataset:\n",
    "    filtered_dataset[key] = WordCleaner.remove_stop_words(dataset[key])\n",
    "datasets.append(filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7120a1",
   "metadata": {},
   "source": [
    "## Remove single letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "no_singles_dataset = {}\n",
    "for key in dataset:\n",
    "    no_singles_dataset[key] = WordCleaner.remove_single_letters(dataset[key])\n",
    "datasets.append(no_singles_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621f9e3",
   "metadata": {},
   "source": [
    "## Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "stemmed_dataset = {}\n",
    "for row in dataset:\n",
    "    stemmed_dataset[row] = WordCleaner.stem(dataset[row], \"Snowball\")\n",
    "datasets.append(stemmed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7a58d",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "lemmad_dataset = {}\n",
    "for row in dataset:\n",
    "    lemmad_dataset[row] = WordCleaner.lemmatize(dataset[row])\n",
    "datasets.append(lemmad_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0bcf1",
   "metadata": {},
   "source": [
    "## Synonym Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = datasets[-1]\n",
    "mapped_2 = {}\n",
    "\n",
    "# Create a pool of workers\n",
    "with Pool() as p:\n",
    "    # Wrap your iterator (dataset) with tqdm for a progress bar\n",
    "    for row in tqdm(dataset):\n",
    "        # Apply the function to each word in the row in parallel\n",
    "        mapped_2[row] = p.map(WordCleaner.get_unified_synonym_2, dataset[row])\n",
    "datasets.append(mapped_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f368df",
   "metadata": {},
   "source": [
    "## Calculating tf-idf for the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b60ed8",
   "metadata": {},
   "source": [
    "### using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = Indexer.calculate_tf_idf(datasets[-1], vectorizer)\n",
    "dataset_keys = list(datasets[-1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdd8aa",
   "metadata": {},
   "source": [
    "# Query Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb756f25",
   "metadata": {},
   "source": [
    "## Manual Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad178",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"wikipedia\"\n",
    "query = word_tokenize(query)\n",
    "query = WordCleaner.remove_stop_words(query)\n",
    "query = WordCleaner.remove_single_letters(query)\n",
    "# query = WordCleaner.stem(query, 'Snowball')\n",
    "query = [WordCleaner.get_unified_synonym_2(word) for word in query]\n",
    "query = WordCleaner.lemmatize(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079d6ac",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859dc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = Indexer.calculate_doc_tf_idf([\" \".join(query)],vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08b71d",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8427af",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_rows = Matcher.get_query_answers(tfidf_matrix, matrix, dataset_keys, 0.35)\n",
    "\n",
    "# Sort the items in the dictionary by value (i.e., rating) in descending order\n",
    "sorted_rows = sorted(similar_rows.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "for row in sorted_rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17659ba6",
   "metadata": {},
   "source": [
    "## Evaluation Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db528201",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = FileManager.csv_to_dict(\"wikir/queries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in queries.keys():\n",
    "    queries[key] = WordCleaner.remove_stop_words(queries[key])\n",
    "for key in queries.keys():\n",
    "    queries[key] = WordCleaner.remove_single_letters(queries[key])\n",
    "for key in queries.keys():\n",
    "    queries[key] = [WordCleaner.get_unified_synonym_2(word) for word in queries[key]]\n",
    "# for key in queries.keys():\n",
    "#     queries[key] = WordCleaner.stem(queries[key], \"Snowball\")\n",
    "for key in queries.keys():\n",
    "    queries[key] = WordCleaner.lemmatize(queries[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320dc0f",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3903f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_matrices = {}\n",
    "for key in queries.keys():\n",
    "    queries_matrices[key] = Indexer.calculate_doc_tf_idf([\" \".join(queries[key])],vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d46388",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562157b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queriesAnswers = {}\n",
    "for key in queries.keys():\n",
    "    queriesAnswers[key] = Matcher.get_query_answers(tfidf_matrix,queries_matrices[key],dataset_keys,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcaba6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_RRM2L_01.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ee075",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"testrun_RRL_01.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_RRML_01.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544defa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_RML_01.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_bert_testing.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71addda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_embedding_5_epoch_1_tfidf_07.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_doc2vec_epoch_1_07.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_doc2vec_epoch_1_05.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aacf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_doc2vec_epoch_9_065.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf78109",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_embedding_5_epoch_15_065.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91bf3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_embedding_5_epoch_15_06.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/testrun_embedding_5_epoch_22_06.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluater.evaluate(\"wikir/qrels\",\"test runs/bert_055.run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3311fe2",
   "metadata": {},
   "source": [
    "# Write To Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ad4ac",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileManager.write_dataset_to_file(\"wikir/RRM2L.csv\",datasets[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a34c7e",
   "metadata": {},
   "source": [
    "## Run File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileManager.write_runfile_to_file(\"test runs/testrun_RRM2L_01.run\",queries,queriesAnswers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbe7ab",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094d4e4",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileManager.write_model_to_drive(\"models/Model_RRM2L\",vectorizer, dataset_keys, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d03817",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da4498",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, svd, dataset_keys, tfidf_matrix = FileManager.load_model_from_drive(\"models/Model_RM_NEW_L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a15f9c",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e64c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "dataset = datasets[-1]\n",
    "# Convert to a list of tokenized documents\n",
    "tokenized_documents = dataset.values()\n",
    "model = Word2Vec(sentences=tokenized_documents,\n",
    "                          vector_size=250,  # Dimensionality of the word vectors (100 is Good for a medium-sized dataset)\n",
    "                          window=5,         # Maximum distance between the current and predicted word within a sentence ( 5 Balances local and broader context)\n",
    "                          sg=1,             # Skip-Gram model (1 for Skip-Gram (can capture complex patterns), 0 for CBOW)\n",
    "                          min_count=1,      # Ignores all words with a total frequency lower than this (2 is Low enough to not lose infrequent words)\n",
    "                          workers = 10\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"embedding_5_epoch_22.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70650f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "dataset = datasets[-1]\n",
    "# Convert to a list of tokenized documents\n",
    "tokenized_documents = dataset.values()\n",
    "model = Word2Vec.load(\"embedding_5_epoch_1.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train(tokenized_documents, total_examples=len(tokenized_documents), epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4843323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "document_vectors = []\n",
    "for doc in tokenized_documents:\n",
    "    # Filter out tokens not in the model's vocabulary\n",
    "    valid_tokens = [token for token in doc if token in model.wv]\n",
    "    # Calculate the average vector for each document\n",
    "    if valid_tokens:  # Check if there are any valid tokens\n",
    "        doc_vector = np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "        document_vectors.append(doc_vector)\n",
    "    else:\n",
    "        # Handle documents with no valid tokens (e.g., empty documents)\n",
    "        document_vectors.append(np.zeros(model.vector_size))\n",
    "\n",
    "# Convert to a 2D array\n",
    "document_vectors = np.array(document_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f15ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_matrices = {}\n",
    "for key, query_tokens in queries.items():\n",
    "    # Filter out tokens not in the model's vocabulary\n",
    "    valid_tokens = [token for token in query_tokens if token in model.wv]\n",
    "\n",
    "    # Calculate the average vector for each query\n",
    "    if valid_tokens:\n",
    "        query_vector = np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "        queries_matrices[key] = query_vector\n",
    "    else:\n",
    "        # Handle queries with no valid tokens\n",
    "        print(\"Query with no valid tokens: \" + key)\n",
    "        queries_matrices[key] = np.zeros(model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "corpus_matrix_sparse = sparse.csr_matrix(document_vectors)\n",
    "queries_answers_embedded = {}\n",
    "\n",
    "for key in queries.keys():\n",
    "    # Reshape the query vector to 2D\n",
    "    query_vector_2d = queries_matrices[key].reshape(1, -1)\n",
    "    # Calculate answers for one query at a time\n",
    "    queries_answers_embedded[key] = Matcher.get_query_answers_optimized(corpus_matrix_sparse, query_vector_2d, dataset_keys, 0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d525019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to run file\n",
    "FileManager.write_runfile_to_file('testrun_embedding_5_epoch_22_06.run', queries, queries_answers_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual query\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a query vector for \"tokyo disney resort\"\n",
    "query_words = [\"apple\"]\n",
    "valid_vectors = [model.wv[word] for word in query_words if word in model.wv]\n",
    "\n",
    "# Check if there are valid vectors to avoid nan issues\n",
    "if valid_vectors:\n",
    "    query_vector = np.mean(valid_vectors, axis=0).reshape(1, -1)\n",
    "    # Compute cosine similarity between query and document vectors\n",
    "    similarity_scores = cosine_similarity(query_vector, document_vectors)\n",
    "\n",
    "    # Rank documents based on similarity scores\n",
    "    sorted_docs = sorted(enumerate(similarity_scores[0]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the ranked documents\n",
    "    for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "        print(f\"Rank {rank}: Document {doc_id + 2} (Similarity Score = {score:.4f})\")\n",
    "else:\n",
    "    print(\"None of the query words were found in the model's vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e2a74",
   "metadata": {},
   "source": [
    "## Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import platform\n",
    "user_os = str(platform.system())\n",
    "\n",
    "history_query = [\"tech\"]\n",
    "query = [\"apple\"]\n",
    "\n",
    "vector_1 = [model.wv[word] for word in query if word in model.wv]\n",
    "vector_2 = [model.wv[word] for word in history_query if word in model.wv]\n",
    "\n",
    "# Check if there are valid vectors to avoid nan issues\n",
    "if vector_1:\n",
    "    query_vector_1 = np.mean(vector_1, axis=0).reshape(1, -1)\n",
    "    query_vector_2 = np.mean(vector_2, axis=0).reshape(1, -1)\n",
    "\n",
    "    # Assuming query_vector_1 and query_vector_2 are already defined as 2D arrays\n",
    "    weighted_vector = 0.75 * query_vector_1 + 0.25 * query_vector_2\n",
    "    # Compute cosine similarity between query and document vectors\n",
    "    similarity_scores = cosine_similarity(weighted_vector, document_vectors)\n",
    "    # Rank documents based on similarity scores\n",
    "    sorted_docs = sorted(enumerate(similarity_scores[0]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the ranked documents\n",
    "    for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "        print(f\"Rank {rank}: Document in row {doc_id + 2} (Similarity Score = {score:.4f})\")\n",
    "else:\n",
    "    print(\"None of the query words were found in the model's vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b50fe7",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d60099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.wv['malaysia'])\n",
    "# print(model.wv.similarity('1st', 'First'))\n",
    "\n",
    "print(model.wv.similarity('world', 'war'))\n",
    "print(model.wv.similarity('good', 'best'))\n",
    "print(model.wv.most_similar('war'))\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e3bb7",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41071cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words is None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.index_to_key), sample)\n",
    "        else:\n",
    "            words = list(model.wv.index_to_key)\n",
    "        \n",
    "    word_vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "    # Determine the appropriate number of components (up to min(n_samples, n_features))\n",
    "    n_components = min(word_vectors.shape[0], word_vectors.shape[1])\n",
    "\n",
    "    if n_components > 1:\n",
    "        twodim = PCA(n_components=n_components).fit_transform(word_vectors)[:, :2]\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='r')\n",
    "        for word, (x, y) in zip(words, twodim):\n",
    "            plt.text(x + 0.05, y + 0.05, word)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Insufficient data for PCA visualization.\")\n",
    "\n",
    "# Example usage:\n",
    "display_pca_scatterplot(model, ['battalion','world', 'war', 'good', 'best', 'state', 'government', 'university', 'college', 'germany', 'german', '12', 'twelve'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0d2f8",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'documents' is a list of preprocessed documents\n",
    "tagged_data = [TaggedDocument(words=datasets[-1].values(), tags=[i]) for i, doc in enumerate(documents)]\n",
    "\n",
    "# Build and train the model\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=2, workers=4, epochs=1)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "# define a list of documents.\n",
    "data = [\"This is the first document\",\n",
    "        \"This is the second document\",\n",
    "        \"This is the third document\",\n",
    "        \"This is the fourth document\"]\n",
    "\n",
    "# get the document vectors\n",
    "document_vectors = [model.infer_vector(word_tokenize(doc.lower())) for doc in data]\n",
    "# Compute cosine similarity between query and document vectors\n",
    "document_vectors = [model.dv[i] for i in range(len(documents))]\n",
    "\n",
    "# Infer a vector for a new document\n",
    "query_vector = model.infer_vector([\"tokyo\", \"disney\", \"resort\"])\n",
    "\n",
    " \n",
    "#  print the document vectors\n",
    "for i, doc in enumerate(data):\n",
    "    print(\"Document\", i+1, \":\", doc)\n",
    "    print(\"Vector:\", document_vectors[i])\n",
    "    print()\n",
    "\n",
    "\n",
    "similarity_scores = cosine_similarity([query_vector], document_vectors)\n",
    "\n",
    "# Use your get_query_answers function to retrieve relevant documents\n",
    "relevant_docs = Matcher.get_query_answers(document_vectors, [query_vector], dataset_key, 0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21787e1a",
   "metadata": {},
   "source": [
    "## Wiki News Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki model\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# print(list(gensim.downloader.info()['models'].keys()))\n",
    "# Load FastText model\n",
    "model = KeyedVectors.load('wiki_news.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937475bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki model 1\n",
    "import numpy as np\n",
    "dataset = datasets[-1]\n",
    "\n",
    "# Convert to a list of tokenized documents\n",
    "tokenized_documents = dataset.values()\n",
    "\n",
    "# Calculate document vectors\n",
    "document_vectors = []\n",
    "for doc in tokenized_documents:\n",
    "    # Filter out tokens not in the model's vocabulary\n",
    "    valid_tokens = [token for token in doc if token in model]\n",
    "    # Calculate the average vector for each document\n",
    "    if valid_tokens:  # Check if there are any valid tokens\n",
    "        doc_vector = np.mean([model[token] for token in valid_tokens], axis=0)\n",
    "        document_vectors.append(doc_vector)\n",
    "    else:\n",
    "        # Handle documents with no valid tokens (e.g., empty documents)\n",
    "        document_vectors.append(np.zeros(model.vector_size))\n",
    "\n",
    "# Convert to a 2D array\n",
    "document_vectors = np.array(document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki model\n",
    "queries_matrices = {}\n",
    "for key, query_tokens in queries.items():\n",
    "    # Filter out tokens not in the model's vocabulary\n",
    "    valid_tokens = [token for token in query_tokens if token in model]\n",
    "\n",
    "    # Calculate the average vector for each query\n",
    "    if valid_tokens:\n",
    "        query_vector = np.mean([model[token] for token in valid_tokens], axis=0)\n",
    "        queries_matrices[key] = query_vector\n",
    "    else:\n",
    "        # Handle queries with no valid tokens\n",
    "        print(\"Query with no valid tokens: \" + key)\n",
    "        queries_matrices[key] = np.zeros(model.vector_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa37a0",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c56dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "dataset = datasets[-1]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize your documents and queries\n",
    "tokenized_documents = {doc_id: tokenizer.tokenize(' '.join(words)) for doc_id, words in dataset.items()}\n",
    "\n",
    "tokenized_queries = {query_id: tokenizer.tokenize(' '.join(words)) for query_id, words in queries.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above cell optimized\n",
    "from transformers import BertTokenizer\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = datasets[-1]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_item(item):\n",
    "    doc_id, words = item\n",
    "    return doc_id, tokenizer.tokenize(' '.join(words))\n",
    "\n",
    "\n",
    "with Pool() as p:\n",
    "    tokenized_documents = dict(tqdm(p.imap(tokenize_item, dataset.items()), total=len(dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = datasets[-1]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize your documents and queries\n",
    "tokenized_documents = {doc_id: tokenizer.tokenize(' '.join(words)) for doc_id, words in tqdm(dataset.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to IDs\n",
    "indexed_documents = {doc_id: tokenizer.convert_tokens_to_ids(words) for doc_id, words in tokenized_documents.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e415e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to IDs\n",
    "indexed_queries = {query_id: tokenizer.convert_tokens_to_ids(words) for query_id, words in tokenized_queries.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate document vectors\n",
    "document_vectors = {doc_id: model(torch.tensor([words]))[0][0][0] for doc_id, words in indexed_documents.items()}\n",
    "\n",
    "query_vectors = {query_id: model(torch.tensor([words]))[0][0][0] for query_id, words in indexed_queries.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6263140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BERT embeddings to 2D numpy arrays\n",
    "document_vectors_np = {doc_id: doc_vector.detach().numpy().reshape(1, -1) for doc_id, doc_vector in document_vectors.items()}\n",
    "query_vectors_np = {query_id: query_vector.detach().numpy().reshape(1, -1) for query_id, query_vector in query_vectors.items()}\n",
    "\n",
    "# Create corpus_matrix and query_matrix\n",
    "corpus_matrix = np.vstack(list(document_vectors_np.values()))\n",
    "query_matrix = np.vstack(list(query_vectors_np.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6710a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queriesAnswers = {}\n",
    "for key in queries.keys():\n",
    "    queriesAnswers[key] = Matcher.get_query_answers_optimized(corpus_matrix, query_matrix, dataset_keys, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac47582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Query\n",
    "query_words = [\"war\"]\n",
    "\n",
    "# Tokenize and convert your query to IDs\n",
    "tokenized_query = tokenizer.tokenize(' '.join(query_words))\n",
    "indexed_query = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
    "\n",
    "# Calculate the query vector\n",
    "query_vector = model(torch.tensor([indexed_query]))[0][0][0]\n",
    "\n",
    "# Convert the query vector to a 2D numpy array\n",
    "query_matrix = query_vector.detach().numpy().reshape(1, -1)\n",
    "\n",
    "# Use your function to get the most similar documents\n",
    "similar_docs = Matcher.get_query_answers_optimized(corpus_matrix, query_matrix, dataset_keys, 0.6)\n",
    "\n",
    "# Print the IDs of the top 5 most similar documents\n",
    "for i, (doc_id, score) in enumerate(list(similar_docs.items())[:5]):\n",
    "    print(f\"Rank {i+1}, Document ID: {doc_id}, Similarity Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    '1781133': ['british', 'tunisia','U.S.', 'USA', 'us', 'United States', 'United-States', 'United_States',\"UK\", \"uk\", \"united\"],\n",
    "}\n",
    "standardized_data = {doc_id: [WordCleaner.standardize_country_names(word) for word in words] for doc_id, words in data.items()}\n",
    "print(standardized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd98a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    '1781133': ['britain', 'ireland','U.S.', 'USA', 'us', 'United States', 'United-States', 'United_States',\"UK\", \"uk\"],\n",
    "}\n",
    "standardized_data = {doc_id: [WordCleaner.standardize_country_names_OLD(word) for word in words] for doc_id, words in data.items()}\n",
    "print(standardized_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
