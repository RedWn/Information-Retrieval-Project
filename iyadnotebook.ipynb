{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ab75af",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import FileManager\n",
    "import WordCleaner\n",
    "import Indexer\n",
    "import Matcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c2740",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d69ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "dataset = FileManager.csv_to_dict('./csv/testing.csv')\n",
    "datasets = [dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bc5d1",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4488bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_file_writer, file = FileManager.open_csv_writer('./csv/filtered.csv',['id','text'])\n",
    "filtered_dataset = {}\n",
    "for row in dataset:\n",
    "    filtered_dataset[row] = WordCleaner.remove_stop_words(dataset[row])\n",
    "    filtered_file_writer.writerow({'id': row, 'text': filtered_dataset[row]})\n",
    "datasets.append(filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621f9e3",
   "metadata": {},
   "source": [
    "## Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "stemmed_file_writer, file = FileManager.open_csv_writer('./csv/stemmed.csv',['id','text'])\n",
    "filtered_stemmed_dataset = {}\n",
    "# To clear the contents of the file\n",
    "for row in dataset:\n",
    "    filtered_stemmed_dataset[row] = WordCleaner.stem(dataset[row], 'Snowball')\n",
    "    stemmed_file_writer.writerow({'id': row, 'text': filtered_stemmed_dataset[row]})\n",
    "datasets.append(filtered_stemmed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7a58d",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "lemmatized_file_writer, file = FileManager.open_csv_writer('./csv/lemmatized.csv',['id','text'])\n",
    "filtered_lemmatized_dataset = {}\n",
    "for row in dataset:\n",
    "    filtered_lemmatized_dataset[row] = WordCleaner.lemmatize(dataset[row])\n",
    "    lemmatized_file_writer.writerow({'id': row, 'text': filtered_lemmatized_dataset[row]})\n",
    "datasets.append(filtered_lemmatized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ae5b0",
   "metadata": {},
   "source": [
    "## Synonym Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[-1]\n",
    "# Build dictionary for the dataset\n",
    "synonym_dict = {}\n",
    "for key, words in dataset.items():\n",
    "    for word in words:\n",
    "        synonym_dict[word] = WordCleaner.get_unified_synonym(word)\n",
    "print(synonym_dict)\n",
    "\n",
    "# Update the dataset with alternative words\n",
    "mapped_dataset = {}\n",
    "for key, words in dataset.items():\n",
    "    mapped_dataset[key] = [synonym_dict[word] for word in words]\n",
    "datasets.append(mapped_dataset)\n",
    "print(mapped_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff081ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_file_writer, file = FileManager.open_csv_writer('./csv/mapped.csv',['id','text'])\n",
    "for row in mapped_dataset:\n",
    "    mapped_file_writer.writerow({'id': row, 'text': mapped_dataset[row]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995bcb4",
   "metadata": {},
   "source": [
    "## Creating the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = Indexer.get_inverted_index(datasets[-1])\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f368df",
   "metadata": {},
   "source": [
    "## Calculating tf-idf for the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tfidf_matrix, df) = Indexer.calculateManualTF_IDF(datasets[-1])\n",
    "# df\n",
    "df = Indexer.calculate_tf_idf(datasets[-1], vectorizer)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56bc2eb",
   "metadata": {},
   "source": [
    "## Query Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2747d",
   "metadata": {},
   "source": [
    "### Process query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'US'\n",
    "query = word_tokenize(query)\n",
    "print(query)\n",
    "query = WordCleaner.remove_stop_words(query)\n",
    "print(query)\n",
    "query = WordCleaner.stem(query, 'porter')\n",
    "print(query)\n",
    "# query = WordCleaner.lemmatize(query)\n",
    "# print(query)\n",
    "query = [WordCleaner.get_alternative(word) for word in query]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608e348",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cded9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens = []\n",
    "# for row in datasets[-1]:\n",
    "#     for token in datasets[-1][row]:\n",
    "#         if token not in all_tokens:\n",
    "#             all_tokens.append(token)\n",
    "                \n",
    "# (query_tfidf_matrix, qdf) = Indexer.calculate_doc_tf_idf(datasets[-1],all_tokens,query)\n",
    "# qdf\n",
    "\n",
    "qdf = Indexer.calculate_doc_tf_idf([' '.join(query)],vectorizer)\n",
    "qdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98f840",
   "metadata": {},
   "source": [
    "### Calculate Cos Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with redwan's work\n",
    "\n",
    "related_docs = Matcher.calcCosSimWithCorpus(df,qdf.iloc[0])\n",
    "\n",
    "# Print the sorted related documents\n",
    "print(\"Related Docs (similarity > 0.5):\")\n",
    "for doc, sim in related_docs:\n",
    "    print(f\"Doc {doc}: Similarity = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2dca66",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df571cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "dataset = datasets[-1]\n",
    "# Convert to a list of tokenized documents\n",
    "tokenized_documents = list(dataset.values())\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_documents,\n",
    "                          vector_size=100,  # Dimensionality of the word vectors\n",
    "                          window=5,         # Maximum distance between the current and predicted word within a sentence\n",
    "                          sg=1,             # Skip-Gram model (1 for Skip-Gram, 0 for CBOW)\n",
    "                          min_count=1,      # Ignores all words with a total frequency lower than this\n",
    "                          workers=4         # Number of CPUS to use\n",
    "                          )       \n",
    "\n",
    "# print(tokenized_documents[5:10])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.train(tokenized_documents, total_examples=len(tokenized_documents), epochs=30)\n",
    "\n",
    "model.save(\"embedding.model\")\n",
    "loaded_model = Word2Vec.load(\"embedding.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of words (vocabulary) from the Word2Vec model\n",
    "words = model.wv.index_to_key\n",
    "print(len(words))\n",
    "print(words)\n",
    "\n",
    "\n",
    "# print(model.wv['malaysia'])\n",
    "# print(model.wv.similarity('1st', 'First'))\n",
    "\n",
    "print(model.wv.similarity('good', 'best'))\n",
    "print(model.wv.similarity('good', 'malaysia'))\n",
    "print(model.wv.most_similar('war'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d53812",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = []\n",
    "for doc in tokenized_documents:\n",
    "    # Calculate the average vector for each document\n",
    "    doc_vector = np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)\n",
    "    document_vectors.append(doc_vector)\n",
    "# print(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f581481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a query vector for \"the united states\"\n",
    "# You can calculate the average vector for the words in your query\n",
    "query_words = [\"machine\"]\n",
    "query_vector = np.mean([model.wv[word] for word in query_words if word in model.wv], axis=0)\n",
    "\n",
    "# Compute cosine similarity between query and document vectors\n",
    "similarity_scores = cosine_similarity([query_vector], document_vectors)\n",
    "\n",
    "# Rank documents based on similarity scores\n",
    "sorted_docs = sorted(enumerate(similarity_scores[0]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the ranked documents\n",
    "for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_id + 2} (Similarity Score = {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5d207",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa1a2",
   "metadata": {},
   "source": [
    "### Draw plot for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words is None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.index_to_key), sample)\n",
    "        else:\n",
    "            words = list(model.wv.index_to_key)\n",
    "        \n",
    "    word_vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "    # Determine the appropriate number of components (up to min(n_samples, n_features))\n",
    "    n_components = min(word_vectors.shape[0], word_vectors.shape[1])\n",
    "\n",
    "    if n_components > 1:\n",
    "        twodim = PCA(n_components=n_components).fit_transform(word_vectors)[:, :2]\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='r')\n",
    "        for word, (x, y) in zip(words, twodim):\n",
    "            plt.text(x + 0.05, y + 0.05, word)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Insufficient data for PCA visualization.\")\n",
    "\n",
    "# Example usage:\n",
    "display_pca_scatterplot(model, ['machine','yoga', 'war', 'good'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa997e",
   "metadata": {},
   "source": [
    "### Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d898c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "\n",
    "# # word = \"henlo cmputr\"  # Incorrect spelling\n",
    "# # print(\"Original text:\", word)\n",
    "# # corrected_word = TextBlob(word).correct()\n",
    "# # print(\"Corrected text:\", corrected_word)\n",
    "\n",
    "# corrected_dataset = {}\n",
    "# stemmed_file_writer, file = FileManager.openCSVWriter('./csv/stemmed.csv',['id','text'])\n",
    "# for row in filtered_stemmed_dataset:\n",
    "#     # Correct spelling using TextBlob\n",
    "#     corrected_dataset[row] = str(TextBlob(str(filtered_stemmed_dataset[row])).correct())\n",
    "#     stemmed_file_writer.writerow({'id': row, 'text': corrected_dataset[row]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80488f",
   "metadata": {},
   "source": [
    "### Synonyms Mapping Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get synonyms for the word 'small'\n",
    "for synset in wordnet.synsets('machine'):\n",
    "    print(synset.name())\n",
    "    print(synset.lemma_names())\n",
    "    \n",
    "sys = wordnet.synsets('car')[0].hypernyms()\n",
    "print(sys[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word, pos):\n",
    "    synonyms = set()\n",
    "    for synset in wordnet.synsets(word, pos):\n",
    "        synonyms.update(synset.lemma_names())\n",
    "    return synonyms\n",
    "\n",
    "# Example usage:\n",
    "synonym_dict = {\n",
    "    \"car\": list(get_synonyms(\"car\", \"n\")),\n",
    "    \"phone\": list(get_synonyms(\"phone\", \"n\")),\n",
    "    # Add more words and their synonyms here...\n",
    "}\n",
    "\n",
    "print(synonym_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3ca51",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 structure\n",
    "# <query_id> Q0 <doc_id> <rank> <BM25_score> BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e89fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from rank_bm25 import BM25Okapi\n",
    "\n",
    "# # Example corpus (list of documents from the DataFrame)\n",
    "# corpus = df_lemma.values.tolist()\n",
    "\n",
    "# # Initialize BM25 model\n",
    "# bm25 = BM25Okapi(corpus)\n",
    "\n",
    "# # Example query\n",
    "# query = \"Introduction Machine Learning Algorithms\"\n",
    "# tokenized_query = query.split()\n",
    "# print(tokenized_query)\n",
    "\n",
    "# # Get BM25 scores for documents\n",
    "# doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# # Write results to BM25.res file\n",
    "# with open(\"BM25.res\", \"w\") as f:\n",
    "#     for rank, (doc_id, score) in enumerate(zip(range(len(doc_scores)), doc_scores)):\n",
    "#         f.write(f\"158491 Q0 {doc_id} {rank} {score:.6f} BM25\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512c367",
   "metadata": {},
   "source": [
    "### Count words in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV file (replace 'your_file.csv' with the actual filename)\n",
    "# df = pd.read_csv('lemmatized.csv')\n",
    "\n",
    "# # Assuming you want to count words in the 'text' column\n",
    "# text_column = df['text']\n",
    "\n",
    "# # Tokenize and count words\n",
    "# total_words = sum(len(text.split()) for text in text_column)\n",
    "\n",
    "# print(f\"Total words in the CSV file: {total_words}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
