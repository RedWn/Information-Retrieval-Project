{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ab75af",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import FileManager\n",
    "import WordCleaner\n",
    "import Indexer\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c2740",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d69ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FileManager.csvToDict('testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bc5d1",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4488bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = {}\n",
    "filtered_file_writer, file = FileManager.openCSVWriter('filtered.csv',['id','text'])\n",
    "for row in dataset:\n",
    "    filtered_dataset[row] = WordCleaner.removeStopWords(dataset[row])\n",
    "    filtered_file_writer.writerow({'id': row, 'text': filtered_dataset[row]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621f9e3",
   "metadata": {},
   "source": [
    "## Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stemmed_dataset = {}\n",
    "# To clear the contents of the file\n",
    "stemmed_file_writer, file = FileManager.openCSVWriter('stemmed.csv',['id','text'])\n",
    "for row in filtered_dataset:\n",
    "    filtered_stemmed_dataset[row] = WordCleaner.stem(filtered_dataset[row], 'porter')\n",
    "    stemmed_file_writer.writerow({'id': row, 'text': filtered_stemmed_dataset[row]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7a58d",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmatized_dataset = {}\n",
    "lemmatized_file_writer, file = FileManager.openCSVWriter('lemmatized.csv',['id','text'])\n",
    "for row in filtered_dataset:\n",
    "    filtered_lemmatized_dataset[row] = WordCleaner.lemmatize(filtered_dataset[row])\n",
    "    lemmatized_file_writer.writerow({'id': row, 'text': filtered_lemmatized_dataset[row]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995bcb4",
   "metadata": {},
   "source": [
    "## Creating the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_inverted_index = Indexer.getInvertedIndex(filtered_stemmed_dataset)\n",
    "stemmed_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c778fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_inverted_index = Indexer.getInvertedIndex(filtered_lemmatized_dataset)\n",
    "lemmatized_inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f368df",
   "metadata": {},
   "source": [
    "## Calculating tf-idf for the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tfidf_matrix, df_stem) = Indexer.calculateTF_IDF(filtered_stemmed_dataset)\n",
    "df_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tfidf_matrix, df_lemma) = Indexer.calculateTF_IDF(filtered_lemmatized_dataset)\n",
    "df_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f55885",
   "metadata": {},
   "source": [
    "### Count words in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV file (replace 'your_file.csv' with the actual filename)\n",
    "# df = pd.read_csv('lemmatized.csv')\n",
    "\n",
    "# # Assuming you want to count words in the 'text' column\n",
    "# text_column = df['text']\n",
    "\n",
    "# # Tokenize and count words\n",
    "# total_words = sum(len(text.split()) for text in text_column)\n",
    "\n",
    "# print(f\"Total words in the CSV file: {total_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa997e",
   "metadata": {},
   "source": [
    "### Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d898c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "\n",
    "# # word = \"henlo cmputr\"  # Incorrect spelling\n",
    "# # print(\"Original text:\", word)\n",
    "# # corrected_word = TextBlob(word).correct()\n",
    "# # print(\"Corrected text:\", corrected_word)\n",
    "\n",
    "# corrected_dataset = {}\n",
    "# stemmed_file_writer, file = FileManager.openCSVWriter('stemmed.csv',['id','text'])\n",
    "# for row in filtered_stemmed_dataset:\n",
    "#     # Correct spelling using TextBlob\n",
    "#     corrected_dataset[row] = str(TextBlob(str(filtered_stemmed_dataset[row])).correct())\n",
    "#     stemmed_file_writer.writerow({'id': row, 'text': corrected_dataset[row]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80488f",
   "metadata": {},
   "source": [
    "### Synonyms Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12711ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonyms_mapping = {\n",
    "#     \"USA\": \"United States of America\",\n",
    "#     \"U.S.\": \"United States of America\",\n",
    "#     \"NYC\": \"New York City\",\n",
    "#     \"1st\": \"first\",\n",
    "#     \"england\": \"britain\"\n",
    "#     # Add more mappings as needed\n",
    "# }\n",
    "\n",
    "\n",
    "# def normalize_term(term):\n",
    "#     return synonyms_mapping.get(term, term)  # Use the canonical form if available, else keep the original term\n",
    "\n",
    "# # Usage:\n",
    "# user_input = \"NYC weather forecast\"\n",
    "# normalized_input = \" \".join(normalize_term(term) for term in user_input.split())\n",
    "# print(normalized_input)  # Output: \"New York City weather forecast\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab890bb",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98012710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "vector_A = df_stem.iloc[0]  # First vector (document 1)\n",
    "\n",
    "# Convert vector A to a numpy array\n",
    "A_array = np.array(vector_A)\n",
    "related_docs = []\n",
    "for index, row in df_stem.iterrows():\n",
    "    B_array = np.array(row)  # Access the row data (vector B)\n",
    "    similarity = cosine_similarity([A_array], [B_array])[0][0]\n",
    "    print(f\"Cosine Similarity between doc 1 and doc {index}: {similarity:.4f}\")\n",
    "    if(similarity > 0.5):\n",
    "        related_docs.append((index, similarity))\n",
    "\n",
    "# Sort related_docs by similarity (highest to lowest)\n",
    "related_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted related documents\n",
    "print(\"Related Docs (similarity > 0.5):\")\n",
    "for doc, sim in related_docs:\n",
    "    print(f\"Doc {doc}: Similarity = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "vector_A = df_lemma.iloc[0]  # First vector (document 1)\n",
    "\n",
    "# Convert vector A to a numpy array\n",
    "A_array = np.array(vector_A)\n",
    "related_docs = []\n",
    "for index, row in df_lemma.iterrows():\n",
    "    B_array = np.array(row)  # Access the row data (vector B)\n",
    "    similarity = cosine_similarity([A_array], [B_array])[0][0]\n",
    "    print(f\"Cosine Similarity between doc 1 and doc {index}: {similarity:.4f}\")\n",
    "    if(similarity > 0.5):\n",
    "        related_docs.append((index, similarity))\n",
    "\n",
    "# Sort related_docs by similarity (highest to lowest)\n",
    "related_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted related documents\n",
    "print(\"Related Docs (similarity > 0.5):\")\n",
    "for doc, sim in related_docs:\n",
    "    print(f\"Doc {doc}: Similarity = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3ca51",
   "metadata": {},
   "source": [
    "## Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 structure\n",
    "# <query_id> Q0 <doc_id> <rank> <BM25_score> BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e89fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from rank_bm25 import BM25Okapi\n",
    "\n",
    "# # Example corpus (list of documents from the DataFrame)\n",
    "# corpus = df_lemma.values.tolist()\n",
    "\n",
    "# # Initialize BM25 model\n",
    "# bm25 = BM25Okapi(corpus)\n",
    "\n",
    "# # Example query\n",
    "# query = \"Introduction Machine Learning Algorithms\"\n",
    "# tokenized_query = query.split()\n",
    "# print(tokenized_query)\n",
    "\n",
    "# # Get BM25 scores for documents\n",
    "# doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# # Write results to BM25.res file\n",
    "# with open(\"BM25.res\", \"w\") as f:\n",
    "#     for rank, (doc_id, score) in enumerate(zip(range(len(doc_scores)), doc_scores)):\n",
    "#         f.write(f\"158491 Q0 {doc_id} {rank} {score:.6f} BM25\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
